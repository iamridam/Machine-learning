{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridam Pal PhD19201\n",
    "# Harsh Bandhey 17234\n",
    "# Vishesh Agrawal 18420"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ridam\\Anaconda3\\lib\\site-packages\\dask\\config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  data = yaml.load(f.read()) or {}\n",
      "C:\\Users\\Ridam\\Anaconda3\\lib\\site-packages\\distributed\\config.py:20: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  defaults = yaml.load(f)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Training files as dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndna_file = 'final_amino_acid_result_ndna.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dna_file = 'final_amino_acid_result_dna.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dna = pd.read_csv(dna_file,skiprows= lambda x: True if x%2 else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndna = pd.read_csv(ndna_file,skiprows= lambda x: True if x%2 else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1299, 21)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dna.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1750, 21)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndna.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## ploting the frequency of labels for DNA and NDNA \n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "label = ['DNA', 'NDNA']\n",
    "freq = [1299,1750]\n",
    "ax.bar(label,freq)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = pd.DataFrame([1 for i in range(1299)]+[0 for i in range(1750)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([dna,ndna])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>AAC_A</th>\n",
       "      <th>AAC_C</th>\n",
       "      <th>AAC_D</th>\n",
       "      <th>AAC_E</th>\n",
       "      <th>AAC_F</th>\n",
       "      <th>AAC_G</th>\n",
       "      <th>AAC_H</th>\n",
       "      <th>AAC_I</th>\n",
       "      <th>AAC_K</th>\n",
       "      <th>...</th>\n",
       "      <th>AAC_M</th>\n",
       "      <th>AAC_N</th>\n",
       "      <th>AAC_P</th>\n",
       "      <th>AAC_Q</th>\n",
       "      <th>AAC_R</th>\n",
       "      <th>AAC_S</th>\n",
       "      <th>AAC_T</th>\n",
       "      <th>AAC_V</th>\n",
       "      <th>AAC_W</th>\n",
       "      <th>AAC_Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D1001</td>\n",
       "      <td>9.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.97</td>\n",
       "      <td>9.42</td>\n",
       "      <td>0.72</td>\n",
       "      <td>7.25</td>\n",
       "      <td>2.17</td>\n",
       "      <td>5.07</td>\n",
       "      <td>4.35</td>\n",
       "      <td>...</td>\n",
       "      <td>4.35</td>\n",
       "      <td>2.90</td>\n",
       "      <td>5.07</td>\n",
       "      <td>2.17</td>\n",
       "      <td>2.90</td>\n",
       "      <td>6.52</td>\n",
       "      <td>4.35</td>\n",
       "      <td>7.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D1005</td>\n",
       "      <td>17.46</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.94</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.59</td>\n",
       "      <td>6.35</td>\n",
       "      <td>3.17</td>\n",
       "      <td>6.35</td>\n",
       "      <td>...</td>\n",
       "      <td>1.59</td>\n",
       "      <td>4.76</td>\n",
       "      <td>3.17</td>\n",
       "      <td>7.94</td>\n",
       "      <td>9.52</td>\n",
       "      <td>7.94</td>\n",
       "      <td>4.76</td>\n",
       "      <td>4.76</td>\n",
       "      <td>1.59</td>\n",
       "      <td>1.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D1008</td>\n",
       "      <td>6.90</td>\n",
       "      <td>6.90</td>\n",
       "      <td>6.90</td>\n",
       "      <td>3.45</td>\n",
       "      <td>3.45</td>\n",
       "      <td>3.45</td>\n",
       "      <td>6.90</td>\n",
       "      <td>6.90</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.90</td>\n",
       "      <td>0.00</td>\n",
       "      <td>20.69</td>\n",
       "      <td>10.34</td>\n",
       "      <td>6.90</td>\n",
       "      <td>3.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D1010</td>\n",
       "      <td>5.43</td>\n",
       "      <td>2.17</td>\n",
       "      <td>2.72</td>\n",
       "      <td>6.52</td>\n",
       "      <td>2.72</td>\n",
       "      <td>8.70</td>\n",
       "      <td>3.26</td>\n",
       "      <td>4.35</td>\n",
       "      <td>8.15</td>\n",
       "      <td>...</td>\n",
       "      <td>2.72</td>\n",
       "      <td>2.17</td>\n",
       "      <td>7.61</td>\n",
       "      <td>5.98</td>\n",
       "      <td>7.07</td>\n",
       "      <td>8.15</td>\n",
       "      <td>3.80</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D1015</td>\n",
       "      <td>6.17</td>\n",
       "      <td>4.94</td>\n",
       "      <td>2.47</td>\n",
       "      <td>3.09</td>\n",
       "      <td>2.47</td>\n",
       "      <td>11.73</td>\n",
       "      <td>6.79</td>\n",
       "      <td>3.70</td>\n",
       "      <td>2.47</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.17</td>\n",
       "      <td>8.02</td>\n",
       "      <td>4.94</td>\n",
       "      <td>4.32</td>\n",
       "      <td>3.09</td>\n",
       "      <td>6.79</td>\n",
       "      <td>8.64</td>\n",
       "      <td>3.09</td>\n",
       "      <td>2.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>D1019</td>\n",
       "      <td>13.27</td>\n",
       "      <td>2.04</td>\n",
       "      <td>4.08</td>\n",
       "      <td>8.16</td>\n",
       "      <td>3.06</td>\n",
       "      <td>3.06</td>\n",
       "      <td>1.02</td>\n",
       "      <td>7.14</td>\n",
       "      <td>7.14</td>\n",
       "      <td>...</td>\n",
       "      <td>1.02</td>\n",
       "      <td>3.06</td>\n",
       "      <td>3.06</td>\n",
       "      <td>1.02</td>\n",
       "      <td>13.27</td>\n",
       "      <td>4.08</td>\n",
       "      <td>3.06</td>\n",
       "      <td>6.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>D1022</td>\n",
       "      <td>11.58</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.16</td>\n",
       "      <td>11.58</td>\n",
       "      <td>1.05</td>\n",
       "      <td>5.26</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.26</td>\n",
       "      <td>10.53</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.11</td>\n",
       "      <td>3.16</td>\n",
       "      <td>1.05</td>\n",
       "      <td>9.47</td>\n",
       "      <td>4.21</td>\n",
       "      <td>5.26</td>\n",
       "      <td>11.58</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>D1025</td>\n",
       "      <td>6.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.58</td>\n",
       "      <td>10.61</td>\n",
       "      <td>3.03</td>\n",
       "      <td>7.58</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.52</td>\n",
       "      <td>21.21</td>\n",
       "      <td>...</td>\n",
       "      <td>4.55</td>\n",
       "      <td>1.52</td>\n",
       "      <td>1.52</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.06</td>\n",
       "      <td>4.55</td>\n",
       "      <td>4.55</td>\n",
       "      <td>10.61</td>\n",
       "      <td>1.52</td>\n",
       "      <td>3.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>D1028</td>\n",
       "      <td>6.80</td>\n",
       "      <td>2.72</td>\n",
       "      <td>4.08</td>\n",
       "      <td>4.08</td>\n",
       "      <td>5.44</td>\n",
       "      <td>11.56</td>\n",
       "      <td>2.04</td>\n",
       "      <td>4.08</td>\n",
       "      <td>4.76</td>\n",
       "      <td>...</td>\n",
       "      <td>2.04</td>\n",
       "      <td>2.72</td>\n",
       "      <td>8.16</td>\n",
       "      <td>2.72</td>\n",
       "      <td>6.80</td>\n",
       "      <td>4.76</td>\n",
       "      <td>6.80</td>\n",
       "      <td>7.48</td>\n",
       "      <td>1.36</td>\n",
       "      <td>2.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>D1032</td>\n",
       "      <td>3.76</td>\n",
       "      <td>1.08</td>\n",
       "      <td>6.45</td>\n",
       "      <td>8.60</td>\n",
       "      <td>2.15</td>\n",
       "      <td>3.23</td>\n",
       "      <td>1.08</td>\n",
       "      <td>4.30</td>\n",
       "      <td>7.53</td>\n",
       "      <td>...</td>\n",
       "      <td>4.84</td>\n",
       "      <td>4.30</td>\n",
       "      <td>2.15</td>\n",
       "      <td>13.44</td>\n",
       "      <td>6.45</td>\n",
       "      <td>4.84</td>\n",
       "      <td>4.30</td>\n",
       "      <td>6.45</td>\n",
       "      <td>1.08</td>\n",
       "      <td>1.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>D1037</td>\n",
       "      <td>4.33</td>\n",
       "      <td>2.76</td>\n",
       "      <td>3.54</td>\n",
       "      <td>5.91</td>\n",
       "      <td>4.33</td>\n",
       "      <td>6.30</td>\n",
       "      <td>1.97</td>\n",
       "      <td>5.91</td>\n",
       "      <td>7.48</td>\n",
       "      <td>...</td>\n",
       "      <td>2.76</td>\n",
       "      <td>7.09</td>\n",
       "      <td>4.33</td>\n",
       "      <td>3.94</td>\n",
       "      <td>3.54</td>\n",
       "      <td>5.91</td>\n",
       "      <td>7.09</td>\n",
       "      <td>7.87</td>\n",
       "      <td>2.76</td>\n",
       "      <td>1.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>D1043</td>\n",
       "      <td>5.67</td>\n",
       "      <td>1.42</td>\n",
       "      <td>2.84</td>\n",
       "      <td>9.22</td>\n",
       "      <td>5.67</td>\n",
       "      <td>6.38</td>\n",
       "      <td>0.71</td>\n",
       "      <td>8.51</td>\n",
       "      <td>7.09</td>\n",
       "      <td>...</td>\n",
       "      <td>2.84</td>\n",
       "      <td>2.84</td>\n",
       "      <td>7.80</td>\n",
       "      <td>3.55</td>\n",
       "      <td>2.84</td>\n",
       "      <td>8.51</td>\n",
       "      <td>6.38</td>\n",
       "      <td>4.26</td>\n",
       "      <td>1.42</td>\n",
       "      <td>4.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>D1047</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.56</td>\n",
       "      <td>9.26</td>\n",
       "      <td>1.85</td>\n",
       "      <td>5.56</td>\n",
       "      <td>5.56</td>\n",
       "      <td>9.26</td>\n",
       "      <td>7.41</td>\n",
       "      <td>...</td>\n",
       "      <td>1.85</td>\n",
       "      <td>1.85</td>\n",
       "      <td>1.85</td>\n",
       "      <td>3.70</td>\n",
       "      <td>5.56</td>\n",
       "      <td>14.81</td>\n",
       "      <td>3.70</td>\n",
       "      <td>1.85</td>\n",
       "      <td>3.70</td>\n",
       "      <td>3.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>D1049</td>\n",
       "      <td>3.23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.61</td>\n",
       "      <td>9.68</td>\n",
       "      <td>6.45</td>\n",
       "      <td>3.23</td>\n",
       "      <td>3.23</td>\n",
       "      <td>3.23</td>\n",
       "      <td>8.06</td>\n",
       "      <td>...</td>\n",
       "      <td>4.84</td>\n",
       "      <td>4.84</td>\n",
       "      <td>6.45</td>\n",
       "      <td>6.45</td>\n",
       "      <td>8.06</td>\n",
       "      <td>4.84</td>\n",
       "      <td>8.06</td>\n",
       "      <td>1.61</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>D1052</td>\n",
       "      <td>3.70</td>\n",
       "      <td>1.23</td>\n",
       "      <td>1.23</td>\n",
       "      <td>7.41</td>\n",
       "      <td>7.41</td>\n",
       "      <td>7.41</td>\n",
       "      <td>1.23</td>\n",
       "      <td>9.88</td>\n",
       "      <td>7.41</td>\n",
       "      <td>...</td>\n",
       "      <td>2.47</td>\n",
       "      <td>2.47</td>\n",
       "      <td>6.17</td>\n",
       "      <td>2.47</td>\n",
       "      <td>6.17</td>\n",
       "      <td>4.94</td>\n",
       "      <td>2.47</td>\n",
       "      <td>9.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>D1055</td>\n",
       "      <td>5.97</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.97</td>\n",
       "      <td>5.97</td>\n",
       "      <td>1.49</td>\n",
       "      <td>5.97</td>\n",
       "      <td>1.49</td>\n",
       "      <td>7.46</td>\n",
       "      <td>13.43</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.48</td>\n",
       "      <td>5.97</td>\n",
       "      <td>7.46</td>\n",
       "      <td>5.97</td>\n",
       "      <td>5.97</td>\n",
       "      <td>1.49</td>\n",
       "      <td>1.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>D1058</td>\n",
       "      <td>7.32</td>\n",
       "      <td>1.22</td>\n",
       "      <td>4.88</td>\n",
       "      <td>7.32</td>\n",
       "      <td>2.44</td>\n",
       "      <td>4.88</td>\n",
       "      <td>2.44</td>\n",
       "      <td>6.10</td>\n",
       "      <td>10.98</td>\n",
       "      <td>...</td>\n",
       "      <td>3.66</td>\n",
       "      <td>8.54</td>\n",
       "      <td>1.22</td>\n",
       "      <td>2.44</td>\n",
       "      <td>6.10</td>\n",
       "      <td>7.32</td>\n",
       "      <td>3.66</td>\n",
       "      <td>7.32</td>\n",
       "      <td>1.22</td>\n",
       "      <td>3.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>D1061</td>\n",
       "      <td>5.75</td>\n",
       "      <td>0.38</td>\n",
       "      <td>6.90</td>\n",
       "      <td>6.13</td>\n",
       "      <td>4.21</td>\n",
       "      <td>8.05</td>\n",
       "      <td>1.92</td>\n",
       "      <td>8.81</td>\n",
       "      <td>7.28</td>\n",
       "      <td>...</td>\n",
       "      <td>2.30</td>\n",
       "      <td>6.90</td>\n",
       "      <td>2.30</td>\n",
       "      <td>2.68</td>\n",
       "      <td>4.98</td>\n",
       "      <td>8.43</td>\n",
       "      <td>3.45</td>\n",
       "      <td>6.13</td>\n",
       "      <td>0.77</td>\n",
       "      <td>3.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>D1067</td>\n",
       "      <td>5.58</td>\n",
       "      <td>0.51</td>\n",
       "      <td>5.58</td>\n",
       "      <td>8.12</td>\n",
       "      <td>5.58</td>\n",
       "      <td>7.61</td>\n",
       "      <td>2.54</td>\n",
       "      <td>5.58</td>\n",
       "      <td>8.12</td>\n",
       "      <td>...</td>\n",
       "      <td>0.51</td>\n",
       "      <td>6.09</td>\n",
       "      <td>4.57</td>\n",
       "      <td>3.55</td>\n",
       "      <td>4.57</td>\n",
       "      <td>3.55</td>\n",
       "      <td>4.57</td>\n",
       "      <td>6.09</td>\n",
       "      <td>3.05</td>\n",
       "      <td>4.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>D1072</td>\n",
       "      <td>5.78</td>\n",
       "      <td>1.73</td>\n",
       "      <td>6.36</td>\n",
       "      <td>8.09</td>\n",
       "      <td>4.62</td>\n",
       "      <td>7.51</td>\n",
       "      <td>2.31</td>\n",
       "      <td>4.62</td>\n",
       "      <td>2.89</td>\n",
       "      <td>...</td>\n",
       "      <td>3.47</td>\n",
       "      <td>2.31</td>\n",
       "      <td>6.36</td>\n",
       "      <td>5.78</td>\n",
       "      <td>5.78</td>\n",
       "      <td>2.89</td>\n",
       "      <td>8.09</td>\n",
       "      <td>5.20</td>\n",
       "      <td>0.58</td>\n",
       "      <td>5.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>D1076</td>\n",
       "      <td>4.88</td>\n",
       "      <td>14.63</td>\n",
       "      <td>2.44</td>\n",
       "      <td>7.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.44</td>\n",
       "      <td>24.39</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.88</td>\n",
       "      <td>7.32</td>\n",
       "      <td>2.44</td>\n",
       "      <td>7.32</td>\n",
       "      <td>7.32</td>\n",
       "      <td>2.44</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.44</td>\n",
       "      <td>2.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>D1078</td>\n",
       "      <td>8.46</td>\n",
       "      <td>0.94</td>\n",
       "      <td>6.27</td>\n",
       "      <td>8.15</td>\n",
       "      <td>3.76</td>\n",
       "      <td>5.33</td>\n",
       "      <td>1.57</td>\n",
       "      <td>8.46</td>\n",
       "      <td>9.40</td>\n",
       "      <td>...</td>\n",
       "      <td>1.25</td>\n",
       "      <td>5.33</td>\n",
       "      <td>3.45</td>\n",
       "      <td>3.13</td>\n",
       "      <td>3.76</td>\n",
       "      <td>5.64</td>\n",
       "      <td>6.27</td>\n",
       "      <td>2.51</td>\n",
       "      <td>1.88</td>\n",
       "      <td>3.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>D1085</td>\n",
       "      <td>5.86</td>\n",
       "      <td>1.85</td>\n",
       "      <td>5.56</td>\n",
       "      <td>4.94</td>\n",
       "      <td>5.86</td>\n",
       "      <td>7.41</td>\n",
       "      <td>2.47</td>\n",
       "      <td>8.95</td>\n",
       "      <td>8.64</td>\n",
       "      <td>...</td>\n",
       "      <td>1.85</td>\n",
       "      <td>6.79</td>\n",
       "      <td>5.25</td>\n",
       "      <td>4.01</td>\n",
       "      <td>4.94</td>\n",
       "      <td>5.56</td>\n",
       "      <td>2.16</td>\n",
       "      <td>4.63</td>\n",
       "      <td>1.23</td>\n",
       "      <td>4.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>D1092</td>\n",
       "      <td>3.59</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.52</td>\n",
       "      <td>5.83</td>\n",
       "      <td>4.48</td>\n",
       "      <td>4.04</td>\n",
       "      <td>2.24</td>\n",
       "      <td>8.97</td>\n",
       "      <td>8.07</td>\n",
       "      <td>...</td>\n",
       "      <td>2.24</td>\n",
       "      <td>6.73</td>\n",
       "      <td>4.48</td>\n",
       "      <td>3.14</td>\n",
       "      <td>4.93</td>\n",
       "      <td>5.38</td>\n",
       "      <td>8.07</td>\n",
       "      <td>6.73</td>\n",
       "      <td>0.90</td>\n",
       "      <td>4.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>D1097</td>\n",
       "      <td>3.01</td>\n",
       "      <td>0.33</td>\n",
       "      <td>7.36</td>\n",
       "      <td>5.35</td>\n",
       "      <td>3.68</td>\n",
       "      <td>7.36</td>\n",
       "      <td>1.00</td>\n",
       "      <td>7.69</td>\n",
       "      <td>6.02</td>\n",
       "      <td>...</td>\n",
       "      <td>3.01</td>\n",
       "      <td>5.69</td>\n",
       "      <td>7.02</td>\n",
       "      <td>4.68</td>\n",
       "      <td>5.35</td>\n",
       "      <td>6.35</td>\n",
       "      <td>4.68</td>\n",
       "      <td>5.35</td>\n",
       "      <td>1.00</td>\n",
       "      <td>4.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>D1103</td>\n",
       "      <td>5.26</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.32</td>\n",
       "      <td>5.26</td>\n",
       "      <td>2.63</td>\n",
       "      <td>7.89</td>\n",
       "      <td>3.95</td>\n",
       "      <td>2.63</td>\n",
       "      <td>5.26</td>\n",
       "      <td>...</td>\n",
       "      <td>1.32</td>\n",
       "      <td>5.26</td>\n",
       "      <td>2.63</td>\n",
       "      <td>3.95</td>\n",
       "      <td>9.21</td>\n",
       "      <td>7.89</td>\n",
       "      <td>6.58</td>\n",
       "      <td>5.26</td>\n",
       "      <td>1.32</td>\n",
       "      <td>7.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>D1106</td>\n",
       "      <td>1.33</td>\n",
       "      <td>12.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>5.33</td>\n",
       "      <td>6.67</td>\n",
       "      <td>2.67</td>\n",
       "      <td>4.00</td>\n",
       "      <td>9.33</td>\n",
       "      <td>...</td>\n",
       "      <td>2.67</td>\n",
       "      <td>6.67</td>\n",
       "      <td>1.33</td>\n",
       "      <td>5.33</td>\n",
       "      <td>9.33</td>\n",
       "      <td>8.00</td>\n",
       "      <td>2.67</td>\n",
       "      <td>8.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>D1109</td>\n",
       "      <td>9.80</td>\n",
       "      <td>1.31</td>\n",
       "      <td>6.54</td>\n",
       "      <td>9.15</td>\n",
       "      <td>3.92</td>\n",
       "      <td>7.19</td>\n",
       "      <td>0.65</td>\n",
       "      <td>5.88</td>\n",
       "      <td>1.31</td>\n",
       "      <td>...</td>\n",
       "      <td>2.61</td>\n",
       "      <td>2.61</td>\n",
       "      <td>3.27</td>\n",
       "      <td>6.54</td>\n",
       "      <td>10.46</td>\n",
       "      <td>5.23</td>\n",
       "      <td>6.54</td>\n",
       "      <td>1.31</td>\n",
       "      <td>1.31</td>\n",
       "      <td>1.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>D1113</td>\n",
       "      <td>5.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.33</td>\n",
       "      <td>10.67</td>\n",
       "      <td>8.00</td>\n",
       "      <td>6.67</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.67</td>\n",
       "      <td>10.67</td>\n",
       "      <td>...</td>\n",
       "      <td>2.67</td>\n",
       "      <td>5.33</td>\n",
       "      <td>2.67</td>\n",
       "      <td>5.33</td>\n",
       "      <td>4.00</td>\n",
       "      <td>6.67</td>\n",
       "      <td>5.33</td>\n",
       "      <td>1.33</td>\n",
       "      <td>1.33</td>\n",
       "      <td>1.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>D1116</td>\n",
       "      <td>8.85</td>\n",
       "      <td>0.66</td>\n",
       "      <td>4.59</td>\n",
       "      <td>6.23</td>\n",
       "      <td>2.62</td>\n",
       "      <td>2.62</td>\n",
       "      <td>5.57</td>\n",
       "      <td>5.25</td>\n",
       "      <td>6.23</td>\n",
       "      <td>...</td>\n",
       "      <td>0.33</td>\n",
       "      <td>4.92</td>\n",
       "      <td>5.57</td>\n",
       "      <td>7.21</td>\n",
       "      <td>7.54</td>\n",
       "      <td>3.61</td>\n",
       "      <td>3.93</td>\n",
       "      <td>7.54</td>\n",
       "      <td>1.64</td>\n",
       "      <td>2.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1720</th>\n",
       "      <td>N5358</td>\n",
       "      <td>11.36</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.41</td>\n",
       "      <td>9.09</td>\n",
       "      <td>5.68</td>\n",
       "      <td>3.41</td>\n",
       "      <td>1.14</td>\n",
       "      <td>6.82</td>\n",
       "      <td>11.36</td>\n",
       "      <td>...</td>\n",
       "      <td>2.27</td>\n",
       "      <td>2.27</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.14</td>\n",
       "      <td>4.55</td>\n",
       "      <td>9.09</td>\n",
       "      <td>4.55</td>\n",
       "      <td>6.82</td>\n",
       "      <td>1.14</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1721</th>\n",
       "      <td>N5360</td>\n",
       "      <td>4.26</td>\n",
       "      <td>9.57</td>\n",
       "      <td>5.32</td>\n",
       "      <td>4.26</td>\n",
       "      <td>4.26</td>\n",
       "      <td>9.57</td>\n",
       "      <td>1.06</td>\n",
       "      <td>8.51</td>\n",
       "      <td>9.57</td>\n",
       "      <td>...</td>\n",
       "      <td>4.26</td>\n",
       "      <td>4.26</td>\n",
       "      <td>6.38</td>\n",
       "      <td>1.06</td>\n",
       "      <td>1.06</td>\n",
       "      <td>4.26</td>\n",
       "      <td>4.26</td>\n",
       "      <td>4.26</td>\n",
       "      <td>1.06</td>\n",
       "      <td>7.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1722</th>\n",
       "      <td>N5362</td>\n",
       "      <td>7.79</td>\n",
       "      <td>4.10</td>\n",
       "      <td>3.28</td>\n",
       "      <td>1.23</td>\n",
       "      <td>5.74</td>\n",
       "      <td>5.33</td>\n",
       "      <td>2.46</td>\n",
       "      <td>5.74</td>\n",
       "      <td>3.28</td>\n",
       "      <td>...</td>\n",
       "      <td>2.87</td>\n",
       "      <td>3.69</td>\n",
       "      <td>4.92</td>\n",
       "      <td>1.64</td>\n",
       "      <td>3.69</td>\n",
       "      <td>10.25</td>\n",
       "      <td>6.15</td>\n",
       "      <td>6.97</td>\n",
       "      <td>2.46</td>\n",
       "      <td>4.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1723</th>\n",
       "      <td>N5364</td>\n",
       "      <td>9.84</td>\n",
       "      <td>1.64</td>\n",
       "      <td>1.64</td>\n",
       "      <td>1.64</td>\n",
       "      <td>16.39</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.20</td>\n",
       "      <td>1.64</td>\n",
       "      <td>...</td>\n",
       "      <td>3.28</td>\n",
       "      <td>6.56</td>\n",
       "      <td>6.56</td>\n",
       "      <td>1.64</td>\n",
       "      <td>1.64</td>\n",
       "      <td>6.56</td>\n",
       "      <td>3.28</td>\n",
       "      <td>9.84</td>\n",
       "      <td>1.64</td>\n",
       "      <td>1.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1724</th>\n",
       "      <td>N5366</td>\n",
       "      <td>9.31</td>\n",
       "      <td>0.68</td>\n",
       "      <td>7.45</td>\n",
       "      <td>7.61</td>\n",
       "      <td>5.25</td>\n",
       "      <td>4.40</td>\n",
       "      <td>1.69</td>\n",
       "      <td>3.05</td>\n",
       "      <td>6.26</td>\n",
       "      <td>...</td>\n",
       "      <td>1.35</td>\n",
       "      <td>6.94</td>\n",
       "      <td>6.09</td>\n",
       "      <td>3.72</td>\n",
       "      <td>6.77</td>\n",
       "      <td>7.45</td>\n",
       "      <td>3.72</td>\n",
       "      <td>6.77</td>\n",
       "      <td>1.52</td>\n",
       "      <td>2.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1725</th>\n",
       "      <td>N5368</td>\n",
       "      <td>7.32</td>\n",
       "      <td>2.51</td>\n",
       "      <td>3.77</td>\n",
       "      <td>3.14</td>\n",
       "      <td>4.60</td>\n",
       "      <td>3.97</td>\n",
       "      <td>3.77</td>\n",
       "      <td>9.00</td>\n",
       "      <td>4.60</td>\n",
       "      <td>...</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.77</td>\n",
       "      <td>6.69</td>\n",
       "      <td>5.44</td>\n",
       "      <td>4.60</td>\n",
       "      <td>9.62</td>\n",
       "      <td>6.28</td>\n",
       "      <td>3.14</td>\n",
       "      <td>0.63</td>\n",
       "      <td>2.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1726</th>\n",
       "      <td>N5370</td>\n",
       "      <td>8.94</td>\n",
       "      <td>1.66</td>\n",
       "      <td>4.30</td>\n",
       "      <td>9.60</td>\n",
       "      <td>2.32</td>\n",
       "      <td>5.96</td>\n",
       "      <td>1.99</td>\n",
       "      <td>6.62</td>\n",
       "      <td>8.94</td>\n",
       "      <td>...</td>\n",
       "      <td>1.99</td>\n",
       "      <td>3.31</td>\n",
       "      <td>3.64</td>\n",
       "      <td>5.96</td>\n",
       "      <td>4.64</td>\n",
       "      <td>5.96</td>\n",
       "      <td>4.97</td>\n",
       "      <td>5.63</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1727</th>\n",
       "      <td>N5372</td>\n",
       "      <td>8.27</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.39</td>\n",
       "      <td>7.49</td>\n",
       "      <td>4.39</td>\n",
       "      <td>5.43</td>\n",
       "      <td>0.52</td>\n",
       "      <td>6.72</td>\n",
       "      <td>6.98</td>\n",
       "      <td>...</td>\n",
       "      <td>1.55</td>\n",
       "      <td>4.13</td>\n",
       "      <td>3.10</td>\n",
       "      <td>7.75</td>\n",
       "      <td>5.17</td>\n",
       "      <td>9.56</td>\n",
       "      <td>3.62</td>\n",
       "      <td>7.75</td>\n",
       "      <td>0.52</td>\n",
       "      <td>3.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1728</th>\n",
       "      <td>N5374</td>\n",
       "      <td>6.44</td>\n",
       "      <td>0.48</td>\n",
       "      <td>5.25</td>\n",
       "      <td>4.30</td>\n",
       "      <td>7.64</td>\n",
       "      <td>5.01</td>\n",
       "      <td>5.01</td>\n",
       "      <td>6.44</td>\n",
       "      <td>5.25</td>\n",
       "      <td>...</td>\n",
       "      <td>1.43</td>\n",
       "      <td>6.44</td>\n",
       "      <td>6.92</td>\n",
       "      <td>4.06</td>\n",
       "      <td>5.01</td>\n",
       "      <td>4.06</td>\n",
       "      <td>5.01</td>\n",
       "      <td>5.97</td>\n",
       "      <td>1.19</td>\n",
       "      <td>5.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1729</th>\n",
       "      <td>N5376</td>\n",
       "      <td>7.37</td>\n",
       "      <td>1.58</td>\n",
       "      <td>4.21</td>\n",
       "      <td>8.95</td>\n",
       "      <td>2.63</td>\n",
       "      <td>14.74</td>\n",
       "      <td>2.63</td>\n",
       "      <td>1.58</td>\n",
       "      <td>3.16</td>\n",
       "      <td>...</td>\n",
       "      <td>1.05</td>\n",
       "      <td>1.05</td>\n",
       "      <td>13.16</td>\n",
       "      <td>6.32</td>\n",
       "      <td>8.42</td>\n",
       "      <td>8.42</td>\n",
       "      <td>3.16</td>\n",
       "      <td>3.16</td>\n",
       "      <td>2.11</td>\n",
       "      <td>1.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1730</th>\n",
       "      <td>N5378</td>\n",
       "      <td>4.01</td>\n",
       "      <td>0.36</td>\n",
       "      <td>2.92</td>\n",
       "      <td>2.92</td>\n",
       "      <td>10.58</td>\n",
       "      <td>4.74</td>\n",
       "      <td>2.19</td>\n",
       "      <td>12.04</td>\n",
       "      <td>5.11</td>\n",
       "      <td>...</td>\n",
       "      <td>4.38</td>\n",
       "      <td>3.65</td>\n",
       "      <td>4.74</td>\n",
       "      <td>3.65</td>\n",
       "      <td>1.09</td>\n",
       "      <td>8.76</td>\n",
       "      <td>2.92</td>\n",
       "      <td>7.30</td>\n",
       "      <td>1.82</td>\n",
       "      <td>2.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1731</th>\n",
       "      <td>N5380</td>\n",
       "      <td>1.57</td>\n",
       "      <td>2.36</td>\n",
       "      <td>1.57</td>\n",
       "      <td>3.15</td>\n",
       "      <td>10.24</td>\n",
       "      <td>1.57</td>\n",
       "      <td>2.36</td>\n",
       "      <td>20.47</td>\n",
       "      <td>10.24</td>\n",
       "      <td>...</td>\n",
       "      <td>4.72</td>\n",
       "      <td>3.15</td>\n",
       "      <td>1.57</td>\n",
       "      <td>1.57</td>\n",
       "      <td>3.15</td>\n",
       "      <td>7.87</td>\n",
       "      <td>5.51</td>\n",
       "      <td>3.94</td>\n",
       "      <td>2.36</td>\n",
       "      <td>4.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1732</th>\n",
       "      <td>N5382</td>\n",
       "      <td>2.26</td>\n",
       "      <td>0.75</td>\n",
       "      <td>2.76</td>\n",
       "      <td>3.51</td>\n",
       "      <td>7.02</td>\n",
       "      <td>3.26</td>\n",
       "      <td>1.25</td>\n",
       "      <td>18.05</td>\n",
       "      <td>9.27</td>\n",
       "      <td>...</td>\n",
       "      <td>1.50</td>\n",
       "      <td>9.02</td>\n",
       "      <td>2.76</td>\n",
       "      <td>3.26</td>\n",
       "      <td>1.50</td>\n",
       "      <td>6.52</td>\n",
       "      <td>6.52</td>\n",
       "      <td>3.76</td>\n",
       "      <td>1.25</td>\n",
       "      <td>3.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1733</th>\n",
       "      <td>N5384</td>\n",
       "      <td>3.39</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.93</td>\n",
       "      <td>3.39</td>\n",
       "      <td>2.54</td>\n",
       "      <td>4.24</td>\n",
       "      <td>1.27</td>\n",
       "      <td>12.29</td>\n",
       "      <td>8.47</td>\n",
       "      <td>...</td>\n",
       "      <td>2.12</td>\n",
       "      <td>13.98</td>\n",
       "      <td>4.66</td>\n",
       "      <td>7.20</td>\n",
       "      <td>1.27</td>\n",
       "      <td>8.90</td>\n",
       "      <td>5.93</td>\n",
       "      <td>4.66</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1734</th>\n",
       "      <td>N5386</td>\n",
       "      <td>3.73</td>\n",
       "      <td>0.83</td>\n",
       "      <td>6.21</td>\n",
       "      <td>5.80</td>\n",
       "      <td>2.90</td>\n",
       "      <td>10.35</td>\n",
       "      <td>1.86</td>\n",
       "      <td>9.32</td>\n",
       "      <td>7.45</td>\n",
       "      <td>...</td>\n",
       "      <td>2.48</td>\n",
       "      <td>5.38</td>\n",
       "      <td>2.28</td>\n",
       "      <td>2.90</td>\n",
       "      <td>4.35</td>\n",
       "      <td>10.14</td>\n",
       "      <td>6.00</td>\n",
       "      <td>5.18</td>\n",
       "      <td>0.62</td>\n",
       "      <td>3.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1735</th>\n",
       "      <td>N5388</td>\n",
       "      <td>4.42</td>\n",
       "      <td>0.18</td>\n",
       "      <td>6.63</td>\n",
       "      <td>7.55</td>\n",
       "      <td>5.16</td>\n",
       "      <td>7.92</td>\n",
       "      <td>2.21</td>\n",
       "      <td>11.60</td>\n",
       "      <td>5.89</td>\n",
       "      <td>...</td>\n",
       "      <td>2.95</td>\n",
       "      <td>2.76</td>\n",
       "      <td>4.42</td>\n",
       "      <td>2.39</td>\n",
       "      <td>7.00</td>\n",
       "      <td>7.37</td>\n",
       "      <td>4.24</td>\n",
       "      <td>6.08</td>\n",
       "      <td>0.18</td>\n",
       "      <td>4.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1736</th>\n",
       "      <td>N5390</td>\n",
       "      <td>8.74</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.94</td>\n",
       "      <td>6.80</td>\n",
       "      <td>3.88</td>\n",
       "      <td>11.65</td>\n",
       "      <td>2.91</td>\n",
       "      <td>0.97</td>\n",
       "      <td>1.94</td>\n",
       "      <td>...</td>\n",
       "      <td>2.91</td>\n",
       "      <td>2.91</td>\n",
       "      <td>4.85</td>\n",
       "      <td>4.85</td>\n",
       "      <td>3.88</td>\n",
       "      <td>6.80</td>\n",
       "      <td>6.80</td>\n",
       "      <td>3.88</td>\n",
       "      <td>4.85</td>\n",
       "      <td>1.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1737</th>\n",
       "      <td>N5392</td>\n",
       "      <td>13.46</td>\n",
       "      <td>0.77</td>\n",
       "      <td>5.00</td>\n",
       "      <td>7.69</td>\n",
       "      <td>3.46</td>\n",
       "      <td>6.92</td>\n",
       "      <td>1.92</td>\n",
       "      <td>5.77</td>\n",
       "      <td>1.54</td>\n",
       "      <td>...</td>\n",
       "      <td>3.08</td>\n",
       "      <td>2.69</td>\n",
       "      <td>5.00</td>\n",
       "      <td>3.85</td>\n",
       "      <td>10.00</td>\n",
       "      <td>6.15</td>\n",
       "      <td>3.85</td>\n",
       "      <td>6.54</td>\n",
       "      <td>0.38</td>\n",
       "      <td>1.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1738</th>\n",
       "      <td>N5394</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.54</td>\n",
       "      <td>2.31</td>\n",
       "      <td>1.93</td>\n",
       "      <td>17.53</td>\n",
       "      <td>1.73</td>\n",
       "      <td>1.73</td>\n",
       "      <td>10.40</td>\n",
       "      <td>14.07</td>\n",
       "      <td>...</td>\n",
       "      <td>1.35</td>\n",
       "      <td>13.87</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.77</td>\n",
       "      <td>1.16</td>\n",
       "      <td>5.59</td>\n",
       "      <td>2.31</td>\n",
       "      <td>2.12</td>\n",
       "      <td>1.54</td>\n",
       "      <td>8.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1739</th>\n",
       "      <td>N5396</td>\n",
       "      <td>5.16</td>\n",
       "      <td>0.47</td>\n",
       "      <td>7.51</td>\n",
       "      <td>7.51</td>\n",
       "      <td>4.69</td>\n",
       "      <td>4.69</td>\n",
       "      <td>0.94</td>\n",
       "      <td>12.68</td>\n",
       "      <td>8.45</td>\n",
       "      <td>...</td>\n",
       "      <td>1.88</td>\n",
       "      <td>3.29</td>\n",
       "      <td>2.82</td>\n",
       "      <td>1.88</td>\n",
       "      <td>7.51</td>\n",
       "      <td>6.10</td>\n",
       "      <td>3.76</td>\n",
       "      <td>5.63</td>\n",
       "      <td>1.88</td>\n",
       "      <td>5.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1740</th>\n",
       "      <td>N5398</td>\n",
       "      <td>1.77</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.62</td>\n",
       "      <td>8.85</td>\n",
       "      <td>1.77</td>\n",
       "      <td>5.31</td>\n",
       "      <td>0.88</td>\n",
       "      <td>8.85</td>\n",
       "      <td>13.27</td>\n",
       "      <td>...</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3.54</td>\n",
       "      <td>2.65</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.62</td>\n",
       "      <td>10.62</td>\n",
       "      <td>4.42</td>\n",
       "      <td>4.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1741</th>\n",
       "      <td>N5400</td>\n",
       "      <td>19.72</td>\n",
       "      <td>1.38</td>\n",
       "      <td>0.92</td>\n",
       "      <td>1.38</td>\n",
       "      <td>5.28</td>\n",
       "      <td>7.57</td>\n",
       "      <td>1.83</td>\n",
       "      <td>5.50</td>\n",
       "      <td>0.69</td>\n",
       "      <td>...</td>\n",
       "      <td>3.21</td>\n",
       "      <td>1.61</td>\n",
       "      <td>3.21</td>\n",
       "      <td>4.36</td>\n",
       "      <td>5.73</td>\n",
       "      <td>7.57</td>\n",
       "      <td>5.05</td>\n",
       "      <td>5.96</td>\n",
       "      <td>1.38</td>\n",
       "      <td>2.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1742</th>\n",
       "      <td>N5402</td>\n",
       "      <td>9.98</td>\n",
       "      <td>1.51</td>\n",
       "      <td>5.46</td>\n",
       "      <td>1.13</td>\n",
       "      <td>3.77</td>\n",
       "      <td>11.30</td>\n",
       "      <td>1.88</td>\n",
       "      <td>5.08</td>\n",
       "      <td>3.95</td>\n",
       "      <td>...</td>\n",
       "      <td>0.94</td>\n",
       "      <td>7.16</td>\n",
       "      <td>5.27</td>\n",
       "      <td>1.88</td>\n",
       "      <td>2.82</td>\n",
       "      <td>8.85</td>\n",
       "      <td>10.73</td>\n",
       "      <td>8.29</td>\n",
       "      <td>0.94</td>\n",
       "      <td>2.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1743</th>\n",
       "      <td>N5404</td>\n",
       "      <td>7.96</td>\n",
       "      <td>0.88</td>\n",
       "      <td>5.31</td>\n",
       "      <td>10.18</td>\n",
       "      <td>3.54</td>\n",
       "      <td>3.10</td>\n",
       "      <td>2.65</td>\n",
       "      <td>7.08</td>\n",
       "      <td>6.64</td>\n",
       "      <td>...</td>\n",
       "      <td>3.54</td>\n",
       "      <td>3.10</td>\n",
       "      <td>4.42</td>\n",
       "      <td>7.52</td>\n",
       "      <td>5.31</td>\n",
       "      <td>3.10</td>\n",
       "      <td>5.31</td>\n",
       "      <td>4.42</td>\n",
       "      <td>2.21</td>\n",
       "      <td>3.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1744</th>\n",
       "      <td>N5406</td>\n",
       "      <td>10.88</td>\n",
       "      <td>0.91</td>\n",
       "      <td>6.65</td>\n",
       "      <td>5.74</td>\n",
       "      <td>2.11</td>\n",
       "      <td>5.74</td>\n",
       "      <td>3.32</td>\n",
       "      <td>9.06</td>\n",
       "      <td>8.76</td>\n",
       "      <td>...</td>\n",
       "      <td>2.11</td>\n",
       "      <td>3.32</td>\n",
       "      <td>3.93</td>\n",
       "      <td>3.63</td>\n",
       "      <td>3.63</td>\n",
       "      <td>3.93</td>\n",
       "      <td>7.85</td>\n",
       "      <td>7.25</td>\n",
       "      <td>0.91</td>\n",
       "      <td>3.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1745</th>\n",
       "      <td>N5408</td>\n",
       "      <td>5.84</td>\n",
       "      <td>1.08</td>\n",
       "      <td>6.06</td>\n",
       "      <td>5.84</td>\n",
       "      <td>5.84</td>\n",
       "      <td>6.49</td>\n",
       "      <td>0.87</td>\n",
       "      <td>4.98</td>\n",
       "      <td>9.09</td>\n",
       "      <td>...</td>\n",
       "      <td>1.95</td>\n",
       "      <td>3.90</td>\n",
       "      <td>3.46</td>\n",
       "      <td>4.11</td>\n",
       "      <td>3.90</td>\n",
       "      <td>9.31</td>\n",
       "      <td>4.33</td>\n",
       "      <td>6.93</td>\n",
       "      <td>0.22</td>\n",
       "      <td>4.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1746</th>\n",
       "      <td>N5410</td>\n",
       "      <td>8.30</td>\n",
       "      <td>2.40</td>\n",
       "      <td>6.99</td>\n",
       "      <td>7.42</td>\n",
       "      <td>3.06</td>\n",
       "      <td>8.52</td>\n",
       "      <td>1.75</td>\n",
       "      <td>6.55</td>\n",
       "      <td>6.77</td>\n",
       "      <td>...</td>\n",
       "      <td>4.37</td>\n",
       "      <td>2.62</td>\n",
       "      <td>3.93</td>\n",
       "      <td>2.84</td>\n",
       "      <td>3.71</td>\n",
       "      <td>6.11</td>\n",
       "      <td>7.21</td>\n",
       "      <td>7.21</td>\n",
       "      <td>1.09</td>\n",
       "      <td>3.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1747</th>\n",
       "      <td>N5412</td>\n",
       "      <td>6.19</td>\n",
       "      <td>3.19</td>\n",
       "      <td>4.19</td>\n",
       "      <td>7.98</td>\n",
       "      <td>4.59</td>\n",
       "      <td>6.19</td>\n",
       "      <td>2.59</td>\n",
       "      <td>4.79</td>\n",
       "      <td>9.18</td>\n",
       "      <td>...</td>\n",
       "      <td>1.40</td>\n",
       "      <td>3.79</td>\n",
       "      <td>3.39</td>\n",
       "      <td>3.79</td>\n",
       "      <td>2.99</td>\n",
       "      <td>11.38</td>\n",
       "      <td>6.59</td>\n",
       "      <td>4.79</td>\n",
       "      <td>1.60</td>\n",
       "      <td>3.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1748</th>\n",
       "      <td>N5414</td>\n",
       "      <td>9.40</td>\n",
       "      <td>3.36</td>\n",
       "      <td>2.68</td>\n",
       "      <td>4.03</td>\n",
       "      <td>1.34</td>\n",
       "      <td>9.40</td>\n",
       "      <td>2.01</td>\n",
       "      <td>2.68</td>\n",
       "      <td>2.68</td>\n",
       "      <td>...</td>\n",
       "      <td>1.34</td>\n",
       "      <td>3.36</td>\n",
       "      <td>8.72</td>\n",
       "      <td>6.04</td>\n",
       "      <td>14.77</td>\n",
       "      <td>8.05</td>\n",
       "      <td>3.36</td>\n",
       "      <td>3.36</td>\n",
       "      <td>3.36</td>\n",
       "      <td>2.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1749</th>\n",
       "      <td>N5416</td>\n",
       "      <td>5.26</td>\n",
       "      <td>3.95</td>\n",
       "      <td>1.32</td>\n",
       "      <td>3.95</td>\n",
       "      <td>5.26</td>\n",
       "      <td>2.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.95</td>\n",
       "      <td>2.63</td>\n",
       "      <td>...</td>\n",
       "      <td>1.32</td>\n",
       "      <td>6.58</td>\n",
       "      <td>2.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.63</td>\n",
       "      <td>9.21</td>\n",
       "      <td>6.58</td>\n",
       "      <td>18.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3049 rows  21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID  AAC_A  AAC_C  AAC_D  AAC_E  AAC_F  AAC_G  AAC_H  AAC_I  AAC_K  \\\n",
       "0     D1001   9.42   0.00   7.97   9.42   0.72   7.25   2.17   5.07   4.35   \n",
       "1     D1005  17.46   1.59   0.00   7.94   0.00   1.59   6.35   3.17   6.35   \n",
       "2     D1008   6.90   6.90   6.90   3.45   3.45   3.45   6.90   6.90   0.00   \n",
       "3     D1010   5.43   2.17   2.72   6.52   2.72   8.70   3.26   4.35   8.15   \n",
       "4     D1015   6.17   4.94   2.47   3.09   2.47  11.73   6.79   3.70   2.47   \n",
       "5     D1019  13.27   2.04   4.08   8.16   3.06   3.06   1.02   7.14   7.14   \n",
       "6     D1022  11.58   0.00   3.16  11.58   1.05   5.26   0.00   5.26  10.53   \n",
       "7     D1025   6.06   0.00   7.58  10.61   3.03   7.58   0.00   1.52  21.21   \n",
       "8     D1028   6.80   2.72   4.08   4.08   5.44  11.56   2.04   4.08   4.76   \n",
       "9     D1032   3.76   1.08   6.45   8.60   2.15   3.23   1.08   4.30   7.53   \n",
       "10    D1037   4.33   2.76   3.54   5.91   4.33   6.30   1.97   5.91   7.48   \n",
       "11    D1043   5.67   1.42   2.84   9.22   5.67   6.38   0.71   8.51   7.09   \n",
       "12    D1047   1.85   0.00   5.56   9.26   1.85   5.56   5.56   9.26   7.41   \n",
       "13    D1049   3.23   0.00   1.61   9.68   6.45   3.23   3.23   3.23   8.06   \n",
       "14    D1052   3.70   1.23   1.23   7.41   7.41   7.41   1.23   9.88   7.41   \n",
       "15    D1055   5.97   0.00   5.97   5.97   1.49   5.97   1.49   7.46  13.43   \n",
       "16    D1058   7.32   1.22   4.88   7.32   2.44   4.88   2.44   6.10  10.98   \n",
       "17    D1061   5.75   0.38   6.90   6.13   4.21   8.05   1.92   8.81   7.28   \n",
       "18    D1067   5.58   0.51   5.58   8.12   5.58   7.61   2.54   5.58   8.12   \n",
       "19    D1072   5.78   1.73   6.36   8.09   4.62   7.51   2.31   4.62   2.89   \n",
       "20    D1076   4.88  14.63   2.44   7.32   0.00   0.00   0.00   2.44  24.39   \n",
       "21    D1078   8.46   0.94   6.27   8.15   3.76   5.33   1.57   8.46   9.40   \n",
       "22    D1085   5.86   1.85   5.56   4.94   5.86   7.41   2.47   8.95   8.64   \n",
       "23    D1092   3.59   0.00   8.52   5.83   4.48   4.04   2.24   8.97   8.07   \n",
       "24    D1097   3.01   0.33   7.36   5.35   3.68   7.36   1.00   7.69   6.02   \n",
       "25    D1103   5.26   0.00   1.32   5.26   2.63   7.89   3.95   2.63   5.26   \n",
       "26    D1106   1.33  12.00   4.00   4.00   5.33   6.67   2.67   4.00   9.33   \n",
       "27    D1109   9.80   1.31   6.54   9.15   3.92   7.19   0.65   5.88   1.31   \n",
       "28    D1113   5.33   0.00   5.33  10.67   8.00   6.67   0.00   2.67  10.67   \n",
       "29    D1116   8.85   0.66   4.59   6.23   2.62   2.62   5.57   5.25   6.23   \n",
       "...     ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "1720  N5358  11.36   0.00   3.41   9.09   5.68   3.41   1.14   6.82  11.36   \n",
       "1721  N5360   4.26   9.57   5.32   4.26   4.26   9.57   1.06   8.51   9.57   \n",
       "1722  N5362   7.79   4.10   3.28   1.23   5.74   5.33   2.46   5.74   3.28   \n",
       "1723  N5364   9.84   1.64   1.64   1.64  16.39   0.00   0.00   8.20   1.64   \n",
       "1724  N5366   9.31   0.68   7.45   7.61   5.25   4.40   1.69   3.05   6.26   \n",
       "1725  N5368   7.32   2.51   3.77   3.14   4.60   3.97   3.77   9.00   4.60   \n",
       "1726  N5370   8.94   1.66   4.30   9.60   2.32   5.96   1.99   6.62   8.94   \n",
       "1727  N5372   8.27   0.00   4.39   7.49   4.39   5.43   0.52   6.72   6.98   \n",
       "1728  N5374   6.44   0.48   5.25   4.30   7.64   5.01   5.01   6.44   5.25   \n",
       "1729  N5376   7.37   1.58   4.21   8.95   2.63  14.74   2.63   1.58   3.16   \n",
       "1730  N5378   4.01   0.36   2.92   2.92  10.58   4.74   2.19  12.04   5.11   \n",
       "1731  N5380   1.57   2.36   1.57   3.15  10.24   1.57   2.36  20.47  10.24   \n",
       "1732  N5382   2.26   0.75   2.76   3.51   7.02   3.26   1.25  18.05   9.27   \n",
       "1733  N5384   3.39   0.00   5.93   3.39   2.54   4.24   1.27  12.29   8.47   \n",
       "1734  N5386   3.73   0.83   6.21   5.80   2.90  10.35   1.86   9.32   7.45   \n",
       "1735  N5388   4.42   0.18   6.63   7.55   5.16   7.92   2.21  11.60   5.89   \n",
       "1736  N5390   8.74   0.00   1.94   6.80   3.88  11.65   2.91   0.97   1.94   \n",
       "1737  N5392  13.46   0.77   5.00   7.69   3.46   6.92   1.92   5.77   1.54   \n",
       "1738  N5394   0.00   1.54   2.31   1.93  17.53   1.73   1.73  10.40  14.07   \n",
       "1739  N5396   5.16   0.47   7.51   7.51   4.69   4.69   0.94  12.68   8.45   \n",
       "1740  N5398   1.77   0.00  10.62   8.85   1.77   5.31   0.88   8.85  13.27   \n",
       "1741  N5400  19.72   1.38   0.92   1.38   5.28   7.57   1.83   5.50   0.69   \n",
       "1742  N5402   9.98   1.51   5.46   1.13   3.77  11.30   1.88   5.08   3.95   \n",
       "1743  N5404   7.96   0.88   5.31  10.18   3.54   3.10   2.65   7.08   6.64   \n",
       "1744  N5406  10.88   0.91   6.65   5.74   2.11   5.74   3.32   9.06   8.76   \n",
       "1745  N5408   5.84   1.08   6.06   5.84   5.84   6.49   0.87   4.98   9.09   \n",
       "1746  N5410   8.30   2.40   6.99   7.42   3.06   8.52   1.75   6.55   6.77   \n",
       "1747  N5412   6.19   3.19   4.19   7.98   4.59   6.19   2.59   4.79   9.18   \n",
       "1748  N5414   9.40   3.36   2.68   4.03   1.34   9.40   2.01   2.68   2.68   \n",
       "1749  N5416   5.26   3.95   1.32   3.95   5.26   2.63   0.00   3.95   2.63   \n",
       "\n",
       "      ...  AAC_M  AAC_N  AAC_P  AAC_Q  AAC_R  AAC_S  AAC_T  AAC_V  AAC_W  \\\n",
       "0     ...   4.35   2.90   5.07   2.17   2.90   6.52   4.35   7.25   0.00   \n",
       "1     ...   1.59   4.76   3.17   7.94   9.52   7.94   4.76   4.76   1.59   \n",
       "2     ...   0.00   0.00   6.90   0.00  20.69  10.34   6.90   3.45   0.00   \n",
       "3     ...   2.72   2.17   7.61   5.98   7.07   8.15   3.80   7.07   0.00   \n",
       "4     ...   0.00   6.17   8.02   4.94   4.32   3.09   6.79   8.64   3.09   \n",
       "5     ...   1.02   3.06   3.06   1.02  13.27   4.08   3.06   6.12   0.00   \n",
       "6     ...   0.00   2.11   3.16   1.05   9.47   4.21   5.26  11.58   0.00   \n",
       "7     ...   4.55   1.52   1.52   0.00   6.06   4.55   4.55  10.61   1.52   \n",
       "8     ...   2.04   2.72   8.16   2.72   6.80   4.76   6.80   7.48   1.36   \n",
       "9     ...   4.84   4.30   2.15  13.44   6.45   4.84   4.30   6.45   1.08   \n",
       "10    ...   2.76   7.09   4.33   3.94   3.54   5.91   7.09   7.87   2.76   \n",
       "11    ...   2.84   2.84   7.80   3.55   2.84   8.51   6.38   4.26   1.42   \n",
       "12    ...   1.85   1.85   1.85   3.70   5.56  14.81   3.70   1.85   3.70   \n",
       "13    ...   4.84   4.84   6.45   6.45   8.06   4.84   8.06   1.61   0.00   \n",
       "14    ...   2.47   2.47   6.17   2.47   6.17   4.94   2.47   9.88   0.00   \n",
       "15    ...   0.00   2.99   0.00   4.48   5.97   7.46   5.97   5.97   1.49   \n",
       "16    ...   3.66   8.54   1.22   2.44   6.10   7.32   3.66   7.32   1.22   \n",
       "17    ...   2.30   6.90   2.30   2.68   4.98   8.43   3.45   6.13   0.77   \n",
       "18    ...   0.51   6.09   4.57   3.55   4.57   3.55   4.57   6.09   3.05   \n",
       "19    ...   3.47   2.31   6.36   5.78   5.78   2.89   8.09   5.20   0.58   \n",
       "20    ...   0.00   4.88   7.32   2.44   7.32   7.32   2.44   0.00   2.44   \n",
       "21    ...   1.25   5.33   3.45   3.13   3.76   5.64   6.27   2.51   1.88   \n",
       "22    ...   1.85   6.79   5.25   4.01   4.94   5.56   2.16   4.63   1.23   \n",
       "23    ...   2.24   6.73   4.48   3.14   4.93   5.38   8.07   6.73   0.90   \n",
       "24    ...   3.01   5.69   7.02   4.68   5.35   6.35   4.68   5.35   1.00   \n",
       "25    ...   1.32   5.26   2.63   3.95   9.21   7.89   6.58   5.26   1.32   \n",
       "26    ...   2.67   6.67   1.33   5.33   9.33   8.00   2.67   8.00   0.00   \n",
       "27    ...   2.61   2.61   3.27   6.54  10.46   5.23   6.54   1.31   1.31   \n",
       "28    ...   2.67   5.33   2.67   5.33   4.00   6.67   5.33   1.33   1.33   \n",
       "29    ...   0.33   4.92   5.57   7.21   7.54   3.61   3.93   7.54   1.64   \n",
       "...   ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "1720  ...   2.27   2.27   5.68   1.14   4.55   9.09   4.55   6.82   1.14   \n",
       "1721  ...   4.26   4.26   6.38   1.06   1.06   4.26   4.26   4.26   1.06   \n",
       "1722  ...   2.87   3.69   4.92   1.64   3.69  10.25   6.15   6.97   2.46   \n",
       "1723  ...   3.28   6.56   6.56   1.64   1.64   6.56   3.28   9.84   1.64   \n",
       "1724  ...   1.35   6.94   6.09   3.72   6.77   7.45   3.72   6.77   1.52   \n",
       "1725  ...   1.05   3.77   6.69   5.44   4.60   9.62   6.28   3.14   0.63   \n",
       "1726  ...   1.99   3.31   3.64   5.96   4.64   5.96   4.97   5.63   0.33   \n",
       "1727  ...   1.55   4.13   3.10   7.75   5.17   9.56   3.62   7.75   0.52   \n",
       "1728  ...   1.43   6.44   6.92   4.06   5.01   4.06   5.01   5.97   1.19   \n",
       "1729  ...   1.05   1.05  13.16   6.32   8.42   8.42   3.16   3.16   2.11   \n",
       "1730  ...   4.38   3.65   4.74   3.65   1.09   8.76   2.92   7.30   1.82   \n",
       "1731  ...   4.72   3.15   1.57   1.57   3.15   7.87   5.51   3.94   2.36   \n",
       "1732  ...   1.50   9.02   2.76   3.26   1.50   6.52   6.52   3.76   1.25   \n",
       "1733  ...   2.12  13.98   4.66   7.20   1.27   8.90   5.93   4.66   0.42   \n",
       "1734  ...   2.48   5.38   2.28   2.90   4.35  10.14   6.00   5.18   0.62   \n",
       "1735  ...   2.95   2.76   4.42   2.39   7.00   7.37   4.24   6.08   0.18   \n",
       "1736  ...   2.91   2.91   4.85   4.85   3.88   6.80   6.80   3.88   4.85   \n",
       "1737  ...   3.08   2.69   5.00   3.85  10.00   6.15   3.85   6.54   0.38   \n",
       "1738  ...   1.35  13.87   0.58   0.77   1.16   5.59   2.31   2.12   1.54   \n",
       "1739  ...   1.88   3.29   2.82   1.88   7.51   6.10   3.76   5.63   1.88   \n",
       "1740  ...   2.65   3.54   2.65   0.00  10.62  10.62   4.42   4.42   0.00   \n",
       "1741  ...   3.21   1.61   3.21   4.36   5.73   7.57   5.05   5.96   1.38   \n",
       "1742  ...   0.94   7.16   5.27   1.88   2.82   8.85  10.73   8.29   0.94   \n",
       "1743  ...   3.54   3.10   4.42   7.52   5.31   3.10   5.31   4.42   2.21   \n",
       "1744  ...   2.11   3.32   3.93   3.63   3.63   3.93   7.85   7.25   0.91   \n",
       "1745  ...   1.95   3.90   3.46   4.11   3.90   9.31   4.33   6.93   0.22   \n",
       "1746  ...   4.37   2.62   3.93   2.84   3.71   6.11   7.21   7.21   1.09   \n",
       "1747  ...   1.40   3.79   3.39   3.79   2.99  11.38   6.59   4.79   1.60   \n",
       "1748  ...   1.34   3.36   8.72   6.04  14.77   8.05   3.36   3.36   3.36   \n",
       "1749  ...   1.32   6.58   2.63   0.00   2.63   9.21   6.58  18.42   0.00   \n",
       "\n",
       "      AAC_Y  \n",
       "0      0.72  \n",
       "1      1.59  \n",
       "2      3.45  \n",
       "3      2.17  \n",
       "4      2.47  \n",
       "5      3.06  \n",
       "6      4.21  \n",
       "7      3.03  \n",
       "8      2.04  \n",
       "9      1.61  \n",
       "10     1.97  \n",
       "11     4.96  \n",
       "12     3.70  \n",
       "13     6.45  \n",
       "14     4.94  \n",
       "15     1.49  \n",
       "16     3.66  \n",
       "17     3.07  \n",
       "18     4.57  \n",
       "19     5.78  \n",
       "20     2.44  \n",
       "21     3.76  \n",
       "22     4.63  \n",
       "23     4.04  \n",
       "24     4.68  \n",
       "25     7.89  \n",
       "26     5.33  \n",
       "27     1.96  \n",
       "28     1.33  \n",
       "29     2.30  \n",
       "...     ...  \n",
       "1720   0.00  \n",
       "1721   7.45  \n",
       "1722   4.51  \n",
       "1723   1.64  \n",
       "1724   2.20  \n",
       "1725   2.51  \n",
       "1726   0.99  \n",
       "1727   3.62  \n",
       "1728   5.73  \n",
       "1729   1.58  \n",
       "1730   2.92  \n",
       "1731   4.72  \n",
       "1732   3.51  \n",
       "1733   0.85  \n",
       "1734   3.11  \n",
       "1735   4.05  \n",
       "1736   1.94  \n",
       "1737   1.92  \n",
       "1738   8.67  \n",
       "1739   5.16  \n",
       "1740   2.65  \n",
       "1741   2.75  \n",
       "1742   2.45  \n",
       "1743   3.10  \n",
       "1744   3.02  \n",
       "1745   4.33  \n",
       "1746   3.49  \n",
       "1747   3.59  \n",
       "1748   2.01  \n",
       "1749   5.26  \n",
       "\n",
       "[3049 rows x 21 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>3039</th>\n",
       "      <th>3040</th>\n",
       "      <th>3041</th>\n",
       "      <th>3042</th>\n",
       "      <th>3043</th>\n",
       "      <th>3044</th>\n",
       "      <th>3045</th>\n",
       "      <th>3046</th>\n",
       "      <th>3047</th>\n",
       "      <th>3048</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows  3049 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0     1     2     3     4     5     6     7     8     9     ...  3039  \\\n",
       "0     1     1     1     1     1     1     1     1     1     1  ...     0   \n",
       "\n",
       "   3040  3041  3042  3043  3044  3045  3046  3047  3048  \n",
       "0     0     0     0     0     0     0     0     0     0  \n",
       "\n",
       "[1 rows x 3049 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## labels for the train data\n",
    "label.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.index = data.ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop('ID',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AAC_A</th>\n",
       "      <th>AAC_C</th>\n",
       "      <th>AAC_D</th>\n",
       "      <th>AAC_E</th>\n",
       "      <th>AAC_F</th>\n",
       "      <th>AAC_G</th>\n",
       "      <th>AAC_H</th>\n",
       "      <th>AAC_I</th>\n",
       "      <th>AAC_K</th>\n",
       "      <th>AAC_L</th>\n",
       "      <th>AAC_M</th>\n",
       "      <th>AAC_N</th>\n",
       "      <th>AAC_P</th>\n",
       "      <th>AAC_Q</th>\n",
       "      <th>AAC_R</th>\n",
       "      <th>AAC_S</th>\n",
       "      <th>AAC_T</th>\n",
       "      <th>AAC_V</th>\n",
       "      <th>AAC_W</th>\n",
       "      <th>AAC_Y</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>D1001</th>\n",
       "      <td>9.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.97</td>\n",
       "      <td>9.42</td>\n",
       "      <td>0.72</td>\n",
       "      <td>7.25</td>\n",
       "      <td>2.17</td>\n",
       "      <td>5.07</td>\n",
       "      <td>4.35</td>\n",
       "      <td>17.39</td>\n",
       "      <td>4.35</td>\n",
       "      <td>2.90</td>\n",
       "      <td>5.07</td>\n",
       "      <td>2.17</td>\n",
       "      <td>2.90</td>\n",
       "      <td>6.52</td>\n",
       "      <td>4.35</td>\n",
       "      <td>7.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D1005</th>\n",
       "      <td>17.46</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.94</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.59</td>\n",
       "      <td>6.35</td>\n",
       "      <td>3.17</td>\n",
       "      <td>6.35</td>\n",
       "      <td>7.94</td>\n",
       "      <td>1.59</td>\n",
       "      <td>4.76</td>\n",
       "      <td>3.17</td>\n",
       "      <td>7.94</td>\n",
       "      <td>9.52</td>\n",
       "      <td>7.94</td>\n",
       "      <td>4.76</td>\n",
       "      <td>4.76</td>\n",
       "      <td>1.59</td>\n",
       "      <td>1.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D1008</th>\n",
       "      <td>6.90</td>\n",
       "      <td>6.90</td>\n",
       "      <td>6.90</td>\n",
       "      <td>3.45</td>\n",
       "      <td>3.45</td>\n",
       "      <td>3.45</td>\n",
       "      <td>6.90</td>\n",
       "      <td>6.90</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.90</td>\n",
       "      <td>0.00</td>\n",
       "      <td>20.69</td>\n",
       "      <td>10.34</td>\n",
       "      <td>6.90</td>\n",
       "      <td>3.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D1010</th>\n",
       "      <td>5.43</td>\n",
       "      <td>2.17</td>\n",
       "      <td>2.72</td>\n",
       "      <td>6.52</td>\n",
       "      <td>2.72</td>\n",
       "      <td>8.70</td>\n",
       "      <td>3.26</td>\n",
       "      <td>4.35</td>\n",
       "      <td>8.15</td>\n",
       "      <td>9.24</td>\n",
       "      <td>2.72</td>\n",
       "      <td>2.17</td>\n",
       "      <td>7.61</td>\n",
       "      <td>5.98</td>\n",
       "      <td>7.07</td>\n",
       "      <td>8.15</td>\n",
       "      <td>3.80</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D1015</th>\n",
       "      <td>6.17</td>\n",
       "      <td>4.94</td>\n",
       "      <td>2.47</td>\n",
       "      <td>3.09</td>\n",
       "      <td>2.47</td>\n",
       "      <td>11.73</td>\n",
       "      <td>6.79</td>\n",
       "      <td>3.70</td>\n",
       "      <td>2.47</td>\n",
       "      <td>8.64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.17</td>\n",
       "      <td>8.02</td>\n",
       "      <td>4.94</td>\n",
       "      <td>4.32</td>\n",
       "      <td>3.09</td>\n",
       "      <td>6.79</td>\n",
       "      <td>8.64</td>\n",
       "      <td>3.09</td>\n",
       "      <td>2.47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       AAC_A  AAC_C  AAC_D  AAC_E  AAC_F  AAC_G  AAC_H  AAC_I  AAC_K  AAC_L  \\\n",
       "ID                                                                            \n",
       "D1001   9.42   0.00   7.97   9.42   0.72   7.25   2.17   5.07   4.35  17.39   \n",
       "D1005  17.46   1.59   0.00   7.94   0.00   1.59   6.35   3.17   6.35   7.94   \n",
       "D1008   6.90   6.90   6.90   3.45   3.45   3.45   6.90   6.90   0.00   3.45   \n",
       "D1010   5.43   2.17   2.72   6.52   2.72   8.70   3.26   4.35   8.15   9.24   \n",
       "D1015   6.17   4.94   2.47   3.09   2.47  11.73   6.79   3.70   2.47   8.64   \n",
       "\n",
       "       AAC_M  AAC_N  AAC_P  AAC_Q  AAC_R  AAC_S  AAC_T  AAC_V  AAC_W  AAC_Y  \n",
       "ID                                                                           \n",
       "D1001   4.35   2.90   5.07   2.17   2.90   6.52   4.35   7.25   0.00   0.72  \n",
       "D1005   1.59   4.76   3.17   7.94   9.52   7.94   4.76   4.76   1.59   1.59  \n",
       "D1008   0.00   0.00   6.90   0.00  20.69  10.34   6.90   3.45   0.00   3.45  \n",
       "D1010   2.72   2.17   7.61   5.98   7.07   8.15   3.80   7.07   0.00   2.17  \n",
       "D1015   0.00   6.17   8.02   4.94   4.32   3.09   6.79   8.64   3.09   2.47  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# head of the final dataframe\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize the features\n",
    "sc = StandardScaler()\n",
    "X_train_std = sc.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.1, max_delta_step=0, max_depth=15,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=400, n_jobs=0, num_parallel_tree=1, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## XGBoost classifier for training the model with best parameter for Grid Search \n",
    "clf = XGBClassifier(learning_rate = 0.1, max_depth = 15, n_estimators = 400)\n",
    "# Train the classifier on data1's feature and target data\n",
    "clf.fit(data, label) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(data,label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding the best parameter using GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "[CV] learning_rate=0.01, max_depth=5, n_estimators=100 ...............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\Users\\Ridam\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  learning_rate=0.01, max_depth=5, n_estimators=100, score=0.667, total=   0.2s\n",
      "[CV] learning_rate=0.01, max_depth=5, n_estimators=100 ...............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  learning_rate=0.01, max_depth=5, n_estimators=100, score=0.651, total=   0.2s\n",
      "[CV] learning_rate=0.01, max_depth=5, n_estimators=100 ...............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.4s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  learning_rate=0.01, max_depth=5, n_estimators=100, score=0.711, total=   0.2s\n",
      "[CV] learning_rate=0.01, max_depth=5, n_estimators=100 ...............\n",
      "[CV]  learning_rate=0.01, max_depth=5, n_estimators=100, score=0.662, total=   0.2s\n",
      "[CV] learning_rate=0.01, max_depth=5, n_estimators=100 ...............\n",
      "[CV]  learning_rate=0.01, max_depth=5, n_estimators=100, score=0.700, total=   0.2s\n",
      "[CV] learning_rate=0.01, max_depth=5, n_estimators=200 ...............\n",
      "[CV]  learning_rate=0.01, max_depth=5, n_estimators=200, score=0.693, total=   0.4s\n",
      "[CV] learning_rate=0.01, max_depth=5, n_estimators=200 ...............\n",
      "[CV]  learning_rate=0.01, max_depth=5, n_estimators=200, score=0.652, total=   0.4s\n",
      "[CV] learning_rate=0.01, max_depth=5, n_estimators=200 ...............\n",
      "[CV]  learning_rate=0.01, max_depth=5, n_estimators=200, score=0.713, total=   0.4s\n",
      "[CV] learning_rate=0.01, max_depth=5, n_estimators=200 ...............\n",
      "[CV]  learning_rate=0.01, max_depth=5, n_estimators=200, score=0.680, total=   0.4s\n",
      "[CV] learning_rate=0.01, max_depth=5, n_estimators=200 ...............\n",
      "[CV]  learning_rate=0.01, max_depth=5, n_estimators=200, score=0.709, total=   0.4s\n",
      "[CV] learning_rate=0.01, max_depth=5, n_estimators=300 ...............\n",
      "[CV]  learning_rate=0.01, max_depth=5, n_estimators=300, score=0.703, total=   0.6s\n",
      "[CV] learning_rate=0.01, max_depth=5, n_estimators=300 ...............\n",
      "[CV]  learning_rate=0.01, max_depth=5, n_estimators=300, score=0.667, total=   0.6s\n",
      "[CV] learning_rate=0.01, max_depth=5, n_estimators=300 ...............\n",
      "[CV]  learning_rate=0.01, max_depth=5, n_estimators=300, score=0.710, total=   0.6s\n",
      "[CV] learning_rate=0.01, max_depth=5, n_estimators=300 ...............\n",
      "[CV]  learning_rate=0.01, max_depth=5, n_estimators=300, score=0.680, total=   0.6s\n",
      "[CV] learning_rate=0.01, max_depth=5, n_estimators=300 ...............\n",
      "[CV]  learning_rate=0.01, max_depth=5, n_estimators=300, score=0.700, total=   0.7s\n",
      "[CV] learning_rate=0.01, max_depth=5, n_estimators=400 ...............\n",
      "[CV]  learning_rate=0.01, max_depth=5, n_estimators=400, score=0.698, total=   0.9s\n",
      "[CV] learning_rate=0.01, max_depth=5, n_estimators=400 ...............\n",
      "[CV]  learning_rate=0.01, max_depth=5, n_estimators=400, score=0.667, total=   0.8s\n",
      "[CV] learning_rate=0.01, max_depth=5, n_estimators=400 ...............\n",
      "[CV]  learning_rate=0.01, max_depth=5, n_estimators=400, score=0.708, total=   0.9s\n",
      "[CV] learning_rate=0.01, max_depth=5, n_estimators=400 ...............\n",
      "[CV]  learning_rate=0.01, max_depth=5, n_estimators=400, score=0.692, total=   0.9s\n",
      "[CV] learning_rate=0.01, max_depth=5, n_estimators=400 ...............\n",
      "[CV]  learning_rate=0.01, max_depth=5, n_estimators=400, score=0.709, total=   0.9s\n",
      "[CV] learning_rate=0.01, max_depth=10, n_estimators=100 ..............\n",
      "[CV]  learning_rate=0.01, max_depth=10, n_estimators=100, score=0.675, total=   0.5s\n",
      "[CV] learning_rate=0.01, max_depth=10, n_estimators=100 ..............\n",
      "[CV]  learning_rate=0.01, max_depth=10, n_estimators=100, score=0.649, total=   0.5s\n",
      "[CV] learning_rate=0.01, max_depth=10, n_estimators=100 ..............\n",
      "[CV]  learning_rate=0.01, max_depth=10, n_estimators=100, score=0.707, total=   0.5s\n",
      "[CV] learning_rate=0.01, max_depth=10, n_estimators=100 ..............\n",
      "[CV]  learning_rate=0.01, max_depth=10, n_estimators=100, score=0.652, total=   0.5s\n",
      "[CV] learning_rate=0.01, max_depth=10, n_estimators=100 ..............\n",
      "[CV]  learning_rate=0.01, max_depth=10, n_estimators=100, score=0.698, total=   0.5s\n",
      "[CV] learning_rate=0.01, max_depth=10, n_estimators=200 ..............\n",
      "[CV]  learning_rate=0.01, max_depth=10, n_estimators=200, score=0.690, total=   1.4s\n",
      "[CV] learning_rate=0.01, max_depth=10, n_estimators=200 ..............\n",
      "[CV]  learning_rate=0.01, max_depth=10, n_estimators=200, score=0.662, total=   1.5s\n",
      "[CV] learning_rate=0.01, max_depth=10, n_estimators=200 ..............\n",
      "[CV]  learning_rate=0.01, max_depth=10, n_estimators=200, score=0.720, total=   1.2s\n",
      "[CV] learning_rate=0.01, max_depth=10, n_estimators=200 ..............\n",
      "[CV]  learning_rate=0.01, max_depth=10, n_estimators=200, score=0.670, total=   1.2s\n",
      "[CV] learning_rate=0.01, max_depth=10, n_estimators=200 ..............\n",
      "[CV]  learning_rate=0.01, max_depth=10, n_estimators=200, score=0.719, total=   1.3s\n",
      "[CV] learning_rate=0.01, max_depth=10, n_estimators=300 ..............\n",
      "[CV]  learning_rate=0.01, max_depth=10, n_estimators=300, score=0.684, total=   2.0s\n",
      "[CV] learning_rate=0.01, max_depth=10, n_estimators=300 ..............\n",
      "[CV]  learning_rate=0.01, max_depth=10, n_estimators=300, score=0.674, total=   2.2s\n",
      "[CV] learning_rate=0.01, max_depth=10, n_estimators=300 ..............\n",
      "[CV]  learning_rate=0.01, max_depth=10, n_estimators=300, score=0.728, total=   1.8s\n",
      "[CV] learning_rate=0.01, max_depth=10, n_estimators=300 ..............\n",
      "[CV]  learning_rate=0.01, max_depth=10, n_estimators=300, score=0.684, total=   2.1s\n",
      "[CV] learning_rate=0.01, max_depth=10, n_estimators=300 ..............\n",
      "[CV]  learning_rate=0.01, max_depth=10, n_estimators=300, score=0.709, total=   1.8s\n",
      "[CV] learning_rate=0.01, max_depth=10, n_estimators=400 ..............\n",
      "[CV]  learning_rate=0.01, max_depth=10, n_estimators=400, score=0.700, total=   2.3s\n",
      "[CV] learning_rate=0.01, max_depth=10, n_estimators=400 ..............\n",
      "[CV]  learning_rate=0.01, max_depth=10, n_estimators=400, score=0.682, total=   2.4s\n",
      "[CV] learning_rate=0.01, max_depth=10, n_estimators=400 ..............\n",
      "[CV]  learning_rate=0.01, max_depth=10, n_estimators=400, score=0.718, total=   2.6s\n",
      "[CV] learning_rate=0.01, max_depth=10, n_estimators=400 ..............\n",
      "[CV]  learning_rate=0.01, max_depth=10, n_estimators=400, score=0.703, total=   3.0s\n",
      "[CV] learning_rate=0.01, max_depth=10, n_estimators=400 ..............\n",
      "[CV]  learning_rate=0.01, max_depth=10, n_estimators=400, score=0.719, total=   2.8s\n",
      "[CV] learning_rate=0.01, max_depth=15, n_estimators=100 ..............\n",
      "[CV]  learning_rate=0.01, max_depth=15, n_estimators=100, score=0.684, total=   1.1s\n",
      "[CV] learning_rate=0.01, max_depth=15, n_estimators=100 ..............\n",
      "[CV]  learning_rate=0.01, max_depth=15, n_estimators=100, score=0.656, total=   0.9s\n",
      "[CV] learning_rate=0.01, max_depth=15, n_estimators=100 ..............\n",
      "[CV]  learning_rate=0.01, max_depth=15, n_estimators=100, score=0.697, total=   0.8s\n",
      "[CV] learning_rate=0.01, max_depth=15, n_estimators=100 ..............\n",
      "[CV]  learning_rate=0.01, max_depth=15, n_estimators=100, score=0.661, total=   0.8s\n",
      "[CV] learning_rate=0.01, max_depth=15, n_estimators=100 ..............\n",
      "[CV]  learning_rate=0.01, max_depth=15, n_estimators=100, score=0.683, total=   1.0s\n",
      "[CV] learning_rate=0.01, max_depth=15, n_estimators=200 ..............\n",
      "[CV]  learning_rate=0.01, max_depth=15, n_estimators=200, score=0.695, total=   1.8s\n",
      "[CV] learning_rate=0.01, max_depth=15, n_estimators=200 ..............\n",
      "[CV]  learning_rate=0.01, max_depth=15, n_estimators=200, score=0.652, total=   1.6s\n",
      "[CV] learning_rate=0.01, max_depth=15, n_estimators=200 ..............\n",
      "[CV]  learning_rate=0.01, max_depth=15, n_estimators=200, score=0.721, total=   1.7s\n",
      "[CV] learning_rate=0.01, max_depth=15, n_estimators=200 ..............\n",
      "[CV]  learning_rate=0.01, max_depth=15, n_estimators=200, score=0.687, total=   1.7s\n",
      "[CV] learning_rate=0.01, max_depth=15, n_estimators=200 ..............\n",
      "[CV]  learning_rate=0.01, max_depth=15, n_estimators=200, score=0.698, total=   1.8s\n",
      "[CV] learning_rate=0.01, max_depth=15, n_estimators=300 ..............\n",
      "[CV]  learning_rate=0.01, max_depth=15, n_estimators=300, score=0.703, total=   2.3s\n",
      "[CV] learning_rate=0.01, max_depth=15, n_estimators=300 ..............\n",
      "[CV]  learning_rate=0.01, max_depth=15, n_estimators=300, score=0.664, total=   2.4s\n",
      "[CV] learning_rate=0.01, max_depth=15, n_estimators=300 ..............\n",
      "[CV]  learning_rate=0.01, max_depth=15, n_estimators=300, score=0.711, total=   2.4s\n",
      "[CV] learning_rate=0.01, max_depth=15, n_estimators=300 ..............\n",
      "[CV]  learning_rate=0.01, max_depth=15, n_estimators=300, score=0.697, total=   2.6s\n",
      "[CV] learning_rate=0.01, max_depth=15, n_estimators=300 ..............\n",
      "[CV]  learning_rate=0.01, max_depth=15, n_estimators=300, score=0.704, total=   2.3s\n",
      "[CV] learning_rate=0.01, max_depth=15, n_estimators=400 ..............\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  learning_rate=0.01, max_depth=15, n_estimators=400, score=0.707, total=   3.4s\n",
      "[CV] learning_rate=0.01, max_depth=15, n_estimators=400 ..............\n",
      "[CV]  learning_rate=0.01, max_depth=15, n_estimators=400, score=0.682, total=   3.4s\n",
      "[CV] learning_rate=0.01, max_depth=15, n_estimators=400 ..............\n",
      "[CV]  learning_rate=0.01, max_depth=15, n_estimators=400, score=0.705, total=   3.4s\n",
      "[CV] learning_rate=0.01, max_depth=15, n_estimators=400 ..............\n",
      "[CV]  learning_rate=0.01, max_depth=15, n_estimators=400, score=0.689, total=   3.4s\n",
      "[CV] learning_rate=0.01, max_depth=15, n_estimators=400 ..............\n",
      "[CV]  learning_rate=0.01, max_depth=15, n_estimators=400, score=0.708, total=   3.1s\n",
      "[CV] learning_rate=0.1, max_depth=5, n_estimators=100 ................\n",
      "[CV]  learning_rate=0.1, max_depth=5, n_estimators=100, score=0.703, total=   0.3s\n",
      "[CV] learning_rate=0.1, max_depth=5, n_estimators=100 ................\n",
      "[CV]  learning_rate=0.1, max_depth=5, n_estimators=100, score=0.680, total=   0.3s\n",
      "[CV] learning_rate=0.1, max_depth=5, n_estimators=100 ................\n",
      "[CV]  learning_rate=0.1, max_depth=5, n_estimators=100, score=0.720, total=   0.3s\n",
      "[CV] learning_rate=0.1, max_depth=5, n_estimators=100 ................\n",
      "[CV]  learning_rate=0.1, max_depth=5, n_estimators=100, score=0.684, total=   0.4s\n",
      "[CV] learning_rate=0.1, max_depth=5, n_estimators=100 ................\n",
      "[CV]  learning_rate=0.1, max_depth=5, n_estimators=100, score=0.706, total=   0.4s\n",
      "[CV] learning_rate=0.1, max_depth=5, n_estimators=200 ................\n",
      "[CV]  learning_rate=0.1, max_depth=5, n_estimators=200, score=0.677, total=   0.6s\n",
      "[CV] learning_rate=0.1, max_depth=5, n_estimators=200 ................\n",
      "[CV]  learning_rate=0.1, max_depth=5, n_estimators=200, score=0.666, total=   0.6s\n",
      "[CV] learning_rate=0.1, max_depth=5, n_estimators=200 ................\n",
      "[CV]  learning_rate=0.1, max_depth=5, n_estimators=200, score=0.721, total=   0.5s\n",
      "[CV] learning_rate=0.1, max_depth=5, n_estimators=200 ................\n",
      "[CV]  learning_rate=0.1, max_depth=5, n_estimators=200, score=0.689, total=   0.5s\n",
      "[CV] learning_rate=0.1, max_depth=5, n_estimators=200 ................\n",
      "[CV]  learning_rate=0.1, max_depth=5, n_estimators=200, score=0.718, total=   0.5s\n",
      "[CV] learning_rate=0.1, max_depth=5, n_estimators=300 ................\n",
      "[CV]  learning_rate=0.1, max_depth=5, n_estimators=300, score=0.682, total=   0.8s\n",
      "[CV] learning_rate=0.1, max_depth=5, n_estimators=300 ................\n",
      "[CV]  learning_rate=0.1, max_depth=5, n_estimators=300, score=0.685, total=   0.9s\n",
      "[CV] learning_rate=0.1, max_depth=5, n_estimators=300 ................\n",
      "[CV]  learning_rate=0.1, max_depth=5, n_estimators=300, score=0.715, total=   0.8s\n",
      "[CV] learning_rate=0.1, max_depth=5, n_estimators=300 ................\n",
      "[CV]  learning_rate=0.1, max_depth=5, n_estimators=300, score=0.693, total=   0.7s\n",
      "[CV] learning_rate=0.1, max_depth=5, n_estimators=300 ................\n",
      "[CV]  learning_rate=0.1, max_depth=5, n_estimators=300, score=0.716, total=   0.8s\n",
      "[CV] learning_rate=0.1, max_depth=5, n_estimators=400 ................\n",
      "[CV]  learning_rate=0.1, max_depth=5, n_estimators=400, score=0.684, total=   1.0s\n",
      "[CV] learning_rate=0.1, max_depth=5, n_estimators=400 ................\n",
      "[CV]  learning_rate=0.1, max_depth=5, n_estimators=400, score=0.689, total=   1.0s\n",
      "[CV] learning_rate=0.1, max_depth=5, n_estimators=400 ................\n",
      "[CV]  learning_rate=0.1, max_depth=5, n_estimators=400, score=0.713, total=   1.0s\n",
      "[CV] learning_rate=0.1, max_depth=5, n_estimators=400 ................\n",
      "[CV]  learning_rate=0.1, max_depth=5, n_estimators=400, score=0.679, total=   1.0s\n",
      "[CV] learning_rate=0.1, max_depth=5, n_estimators=400 ................\n",
      "[CV]  learning_rate=0.1, max_depth=5, n_estimators=400, score=0.716, total=   1.0s\n",
      "[CV] learning_rate=0.1, max_depth=10, n_estimators=100 ...............\n",
      "[CV]  learning_rate=0.1, max_depth=10, n_estimators=100, score=0.703, total=   0.5s\n",
      "[CV] learning_rate=0.1, max_depth=10, n_estimators=100 ...............\n",
      "[CV]  learning_rate=0.1, max_depth=10, n_estimators=100, score=0.689, total=   0.5s\n",
      "[CV] learning_rate=0.1, max_depth=10, n_estimators=100 ...............\n",
      "[CV]  learning_rate=0.1, max_depth=10, n_estimators=100, score=0.703, total=   0.5s\n",
      "[CV] learning_rate=0.1, max_depth=10, n_estimators=100 ...............\n",
      "[CV]  learning_rate=0.1, max_depth=10, n_estimators=100, score=0.685, total=   0.5s\n",
      "[CV] learning_rate=0.1, max_depth=10, n_estimators=100 ...............\n",
      "[CV]  learning_rate=0.1, max_depth=10, n_estimators=100, score=0.708, total=   0.5s\n",
      "[CV] learning_rate=0.1, max_depth=10, n_estimators=200 ...............\n",
      "[CV]  learning_rate=0.1, max_depth=10, n_estimators=200, score=0.702, total=   0.9s\n",
      "[CV] learning_rate=0.1, max_depth=10, n_estimators=200 ...............\n",
      "[CV]  learning_rate=0.1, max_depth=10, n_estimators=200, score=0.690, total=   1.0s\n",
      "[CV] learning_rate=0.1, max_depth=10, n_estimators=200 ...............\n",
      "[CV]  learning_rate=0.1, max_depth=10, n_estimators=200, score=0.718, total=   1.0s\n",
      "[CV] learning_rate=0.1, max_depth=10, n_estimators=200 ...............\n",
      "[CV]  learning_rate=0.1, max_depth=10, n_estimators=200, score=0.687, total=   0.9s\n",
      "[CV] learning_rate=0.1, max_depth=10, n_estimators=200 ...............\n",
      "[CV]  learning_rate=0.1, max_depth=10, n_estimators=200, score=0.711, total=   0.9s\n",
      "[CV] learning_rate=0.1, max_depth=10, n_estimators=300 ...............\n",
      "[CV]  learning_rate=0.1, max_depth=10, n_estimators=300, score=0.710, total=   1.5s\n",
      "[CV] learning_rate=0.1, max_depth=10, n_estimators=300 ...............\n",
      "[CV]  learning_rate=0.1, max_depth=10, n_estimators=300, score=0.698, total=   1.6s\n",
      "[CV] learning_rate=0.1, max_depth=10, n_estimators=300 ...............\n",
      "[CV]  learning_rate=0.1, max_depth=10, n_estimators=300, score=0.710, total=   1.4s\n",
      "[CV] learning_rate=0.1, max_depth=10, n_estimators=300 ...............\n",
      "[CV]  learning_rate=0.1, max_depth=10, n_estimators=300, score=0.677, total=   1.4s\n",
      "[CV] learning_rate=0.1, max_depth=10, n_estimators=300 ...............\n",
      "[CV]  learning_rate=0.1, max_depth=10, n_estimators=300, score=0.709, total=   1.4s\n",
      "[CV] learning_rate=0.1, max_depth=10, n_estimators=400 ...............\n",
      "[CV]  learning_rate=0.1, max_depth=10, n_estimators=400, score=0.711, total=   1.6s\n",
      "[CV] learning_rate=0.1, max_depth=10, n_estimators=400 ...............\n",
      "[CV]  learning_rate=0.1, max_depth=10, n_estimators=400, score=0.702, total=   1.8s\n",
      "[CV] learning_rate=0.1, max_depth=10, n_estimators=400 ...............\n",
      "[CV]  learning_rate=0.1, max_depth=10, n_estimators=400, score=0.710, total=   1.8s\n",
      "[CV] learning_rate=0.1, max_depth=10, n_estimators=400 ...............\n",
      "[CV]  learning_rate=0.1, max_depth=10, n_estimators=400, score=0.687, total=   1.7s\n",
      "[CV] learning_rate=0.1, max_depth=10, n_estimators=400 ...............\n",
      "[CV]  learning_rate=0.1, max_depth=10, n_estimators=400, score=0.706, total=   1.6s\n",
      "[CV] learning_rate=0.1, max_depth=15, n_estimators=100 ...............\n",
      "[CV]  learning_rate=0.1, max_depth=15, n_estimators=100, score=0.718, total=   0.8s\n",
      "[CV] learning_rate=0.1, max_depth=15, n_estimators=100 ...............\n",
      "[CV]  learning_rate=0.1, max_depth=15, n_estimators=100, score=0.667, total=   0.8s\n",
      "[CV] learning_rate=0.1, max_depth=15, n_estimators=100 ...............\n",
      "[CV]  learning_rate=0.1, max_depth=15, n_estimators=100, score=0.728, total=   0.7s\n",
      "[CV] learning_rate=0.1, max_depth=15, n_estimators=100 ...............\n",
      "[CV]  learning_rate=0.1, max_depth=15, n_estimators=100, score=0.682, total=   0.7s\n",
      "[CV] learning_rate=0.1, max_depth=15, n_estimators=100 ...............\n",
      "[CV]  learning_rate=0.1, max_depth=15, n_estimators=100, score=0.714, total=   0.7s\n",
      "[CV] learning_rate=0.1, max_depth=15, n_estimators=200 ...............\n",
      "[CV]  learning_rate=0.1, max_depth=15, n_estimators=200, score=0.718, total=   1.1s\n",
      "[CV] learning_rate=0.1, max_depth=15, n_estimators=200 ...............\n",
      "[CV]  learning_rate=0.1, max_depth=15, n_estimators=200, score=0.695, total=   1.1s\n",
      "[CV] learning_rate=0.1, max_depth=15, n_estimators=200 ...............\n",
      "[CV]  learning_rate=0.1, max_depth=15, n_estimators=200, score=0.721, total=   1.1s\n",
      "[CV] learning_rate=0.1, max_depth=15, n_estimators=200 ...............\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  learning_rate=0.1, max_depth=15, n_estimators=200, score=0.697, total=   1.1s\n",
      "[CV] learning_rate=0.1, max_depth=15, n_estimators=200 ...............\n",
      "[CV]  learning_rate=0.1, max_depth=15, n_estimators=200, score=0.698, total=   1.1s\n",
      "[CV] learning_rate=0.1, max_depth=15, n_estimators=300 ...............\n",
      "[CV]  learning_rate=0.1, max_depth=15, n_estimators=300, score=0.716, total=   1.5s\n",
      "[CV] learning_rate=0.1, max_depth=15, n_estimators=300 ...............\n",
      "[CV]  learning_rate=0.1, max_depth=15, n_estimators=300, score=0.687, total=   1.4s\n",
      "[CV] learning_rate=0.1, max_depth=15, n_estimators=300 ...............\n",
      "[CV]  learning_rate=0.1, max_depth=15, n_estimators=300, score=0.718, total=   1.6s\n",
      "[CV] learning_rate=0.1, max_depth=15, n_estimators=300 ...............\n",
      "[CV]  learning_rate=0.1, max_depth=15, n_estimators=300, score=0.703, total=   1.5s\n",
      "[CV] learning_rate=0.1, max_depth=15, n_estimators=300 ...............\n",
      "[CV]  learning_rate=0.1, max_depth=15, n_estimators=300, score=0.708, total=   1.5s\n",
      "[CV] learning_rate=0.1, max_depth=15, n_estimators=400 ...............\n",
      "[CV]  learning_rate=0.1, max_depth=15, n_estimators=400, score=0.713, total=   2.0s\n",
      "[CV] learning_rate=0.1, max_depth=15, n_estimators=400 ...............\n",
      "[CV]  learning_rate=0.1, max_depth=15, n_estimators=400, score=0.685, total=   1.8s\n",
      "[CV] learning_rate=0.1, max_depth=15, n_estimators=400 ...............\n",
      "[CV]  learning_rate=0.1, max_depth=15, n_estimators=400, score=0.716, total=   1.8s\n",
      "[CV] learning_rate=0.1, max_depth=15, n_estimators=400 ...............\n",
      "[CV]  learning_rate=0.1, max_depth=15, n_estimators=400, score=0.708, total=   2.0s\n",
      "[CV] learning_rate=0.1, max_depth=15, n_estimators=400 ...............\n",
      "[CV]  learning_rate=0.1, max_depth=15, n_estimators=400, score=0.711, total=   2.0s\n",
      "[CV] learning_rate=1.0, max_depth=5, n_estimators=100 ................\n",
      "[CV]  learning_rate=1.0, max_depth=5, n_estimators=100, score=0.670, total=   0.3s\n",
      "[CV] learning_rate=1.0, max_depth=5, n_estimators=100 ................\n",
      "[CV]  learning_rate=1.0, max_depth=5, n_estimators=100, score=0.641, total=   0.3s\n",
      "[CV] learning_rate=1.0, max_depth=5, n_estimators=100 ................\n",
      "[CV]  learning_rate=1.0, max_depth=5, n_estimators=100, score=0.707, total=   0.3s\n",
      "[CV] learning_rate=1.0, max_depth=5, n_estimators=100 ................\n",
      "[CV]  learning_rate=1.0, max_depth=5, n_estimators=100, score=0.684, total=   0.3s\n",
      "[CV] learning_rate=1.0, max_depth=5, n_estimators=100 ................\n",
      "[CV]  learning_rate=1.0, max_depth=5, n_estimators=100, score=0.700, total=   0.3s\n",
      "[CV] learning_rate=1.0, max_depth=5, n_estimators=200 ................\n",
      "[CV]  learning_rate=1.0, max_depth=5, n_estimators=200, score=0.684, total=   0.5s\n",
      "[CV] learning_rate=1.0, max_depth=5, n_estimators=200 ................\n",
      "[CV]  learning_rate=1.0, max_depth=5, n_estimators=200, score=0.652, total=   0.5s\n",
      "[CV] learning_rate=1.0, max_depth=5, n_estimators=200 ................\n",
      "[CV]  learning_rate=1.0, max_depth=5, n_estimators=200, score=0.702, total=   0.5s\n",
      "[CV] learning_rate=1.0, max_depth=5, n_estimators=200 ................\n",
      "[CV]  learning_rate=1.0, max_depth=5, n_estimators=200, score=0.684, total=   0.6s\n",
      "[CV] learning_rate=1.0, max_depth=5, n_estimators=200 ................\n",
      "[CV]  learning_rate=1.0, max_depth=5, n_estimators=200, score=0.701, total=   0.5s\n",
      "[CV] learning_rate=1.0, max_depth=5, n_estimators=300 ................\n",
      "[CV]  learning_rate=1.0, max_depth=5, n_estimators=300, score=0.677, total=   0.6s\n",
      "[CV] learning_rate=1.0, max_depth=5, n_estimators=300 ................\n",
      "[CV]  learning_rate=1.0, max_depth=5, n_estimators=300, score=0.656, total=   0.6s\n",
      "[CV] learning_rate=1.0, max_depth=5, n_estimators=300 ................\n",
      "[CV]  learning_rate=1.0, max_depth=5, n_estimators=300, score=0.698, total=   0.6s\n",
      "[CV] learning_rate=1.0, max_depth=5, n_estimators=300 ................\n",
      "[CV]  learning_rate=1.0, max_depth=5, n_estimators=300, score=0.689, total=   0.6s\n",
      "[CV] learning_rate=1.0, max_depth=5, n_estimators=300 ................\n",
      "[CV]  learning_rate=1.0, max_depth=5, n_estimators=300, score=0.691, total=   0.7s\n",
      "[CV] learning_rate=1.0, max_depth=5, n_estimators=400 ................\n",
      "[CV]  learning_rate=1.0, max_depth=5, n_estimators=400, score=0.672, total=   0.7s\n",
      "[CV] learning_rate=1.0, max_depth=5, n_estimators=400 ................\n",
      "[CV]  learning_rate=1.0, max_depth=5, n_estimators=400, score=0.664, total=   0.7s\n",
      "[CV] learning_rate=1.0, max_depth=5, n_estimators=400 ................\n",
      "[CV]  learning_rate=1.0, max_depth=5, n_estimators=400, score=0.698, total=   0.8s\n",
      "[CV] learning_rate=1.0, max_depth=5, n_estimators=400 ................\n",
      "[CV]  learning_rate=1.0, max_depth=5, n_estimators=400, score=0.687, total=   0.7s\n",
      "[CV] learning_rate=1.0, max_depth=5, n_estimators=400 ................\n",
      "[CV]  learning_rate=1.0, max_depth=5, n_estimators=400, score=0.696, total=   0.8s\n",
      "[CV] learning_rate=1.0, max_depth=10, n_estimators=100 ...............\n",
      "[CV]  learning_rate=1.0, max_depth=10, n_estimators=100, score=0.693, total=   0.4s\n",
      "[CV] learning_rate=1.0, max_depth=10, n_estimators=100 ...............\n",
      "[CV]  learning_rate=1.0, max_depth=10, n_estimators=100, score=0.680, total=   0.4s\n",
      "[CV] learning_rate=1.0, max_depth=10, n_estimators=100 ...............\n",
      "[CV]  learning_rate=1.0, max_depth=10, n_estimators=100, score=0.693, total=   0.4s\n",
      "[CV] learning_rate=1.0, max_depth=10, n_estimators=100 ...............\n",
      "[CV]  learning_rate=1.0, max_depth=10, n_estimators=100, score=0.662, total=   0.3s\n",
      "[CV] learning_rate=1.0, max_depth=10, n_estimators=100 ...............\n",
      "[CV]  learning_rate=1.0, max_depth=10, n_estimators=100, score=0.686, total=   0.3s\n",
      "[CV] learning_rate=1.0, max_depth=10, n_estimators=200 ...............\n",
      "[CV]  learning_rate=1.0, max_depth=10, n_estimators=200, score=0.695, total=   0.6s\n",
      "[CV] learning_rate=1.0, max_depth=10, n_estimators=200 ...............\n",
      "[CV]  learning_rate=1.0, max_depth=10, n_estimators=200, score=0.679, total=   0.5s\n",
      "[CV] learning_rate=1.0, max_depth=10, n_estimators=200 ...............\n",
      "[CV]  learning_rate=1.0, max_depth=10, n_estimators=200, score=0.715, total=   0.5s\n",
      "[CV] learning_rate=1.0, max_depth=10, n_estimators=200 ...............\n",
      "[CV]  learning_rate=1.0, max_depth=10, n_estimators=200, score=0.656, total=   0.5s\n",
      "[CV] learning_rate=1.0, max_depth=10, n_estimators=200 ...............\n",
      "[CV]  learning_rate=1.0, max_depth=10, n_estimators=200, score=0.698, total=   0.4s\n",
      "[CV] learning_rate=1.0, max_depth=10, n_estimators=300 ...............\n",
      "[CV]  learning_rate=1.0, max_depth=10, n_estimators=300, score=0.687, total=   0.6s\n",
      "[CV] learning_rate=1.0, max_depth=10, n_estimators=300 ...............\n",
      "[CV]  learning_rate=1.0, max_depth=10, n_estimators=300, score=0.675, total=   0.6s\n",
      "[CV] learning_rate=1.0, max_depth=10, n_estimators=300 ...............\n",
      "[CV]  learning_rate=1.0, max_depth=10, n_estimators=300, score=0.698, total=   0.6s\n",
      "[CV] learning_rate=1.0, max_depth=10, n_estimators=300 ...............\n",
      "[CV]  learning_rate=1.0, max_depth=10, n_estimators=300, score=0.656, total=   0.6s\n",
      "[CV] learning_rate=1.0, max_depth=10, n_estimators=300 ...............\n",
      "[CV]  learning_rate=1.0, max_depth=10, n_estimators=300, score=0.703, total=   0.6s\n",
      "[CV] learning_rate=1.0, max_depth=10, n_estimators=400 ...............\n",
      "[CV]  learning_rate=1.0, max_depth=10, n_estimators=400, score=0.692, total=   0.8s\n",
      "[CV] learning_rate=1.0, max_depth=10, n_estimators=400 ...............\n",
      "[CV]  learning_rate=1.0, max_depth=10, n_estimators=400, score=0.670, total=   0.8s\n",
      "[CV] learning_rate=1.0, max_depth=10, n_estimators=400 ...............\n",
      "[CV]  learning_rate=1.0, max_depth=10, n_estimators=400, score=0.693, total=   0.7s\n",
      "[CV] learning_rate=1.0, max_depth=10, n_estimators=400 ...............\n",
      "[CV]  learning_rate=1.0, max_depth=10, n_estimators=400, score=0.661, total=   0.7s\n",
      "[CV] learning_rate=1.0, max_depth=10, n_estimators=400 ...............\n",
      "[CV]  learning_rate=1.0, max_depth=10, n_estimators=400, score=0.698, total=   0.7s\n",
      "[CV] learning_rate=1.0, max_depth=15, n_estimators=100 ...............\n",
      "[CV]  learning_rate=1.0, max_depth=15, n_estimators=100, score=0.703, total=   0.3s\n",
      "[CV] learning_rate=1.0, max_depth=15, n_estimators=100 ...............\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  learning_rate=1.0, max_depth=15, n_estimators=100, score=0.654, total=   0.4s\n",
      "[CV] learning_rate=1.0, max_depth=15, n_estimators=100 ...............\n",
      "[CV]  learning_rate=1.0, max_depth=15, n_estimators=100, score=0.703, total=   0.4s\n",
      "[CV] learning_rate=1.0, max_depth=15, n_estimators=100 ...............\n",
      "[CV]  learning_rate=1.0, max_depth=15, n_estimators=100, score=0.674, total=   0.4s\n",
      "[CV] learning_rate=1.0, max_depth=15, n_estimators=100 ...............\n",
      "[CV]  learning_rate=1.0, max_depth=15, n_estimators=100, score=0.721, total=   0.3s\n",
      "[CV] learning_rate=1.0, max_depth=15, n_estimators=200 ...............\n",
      "[CV]  learning_rate=1.0, max_depth=15, n_estimators=200, score=0.705, total=   0.5s\n",
      "[CV] learning_rate=1.0, max_depth=15, n_estimators=200 ...............\n",
      "[CV]  learning_rate=1.0, max_depth=15, n_estimators=200, score=0.644, total=   0.5s\n",
      "[CV] learning_rate=1.0, max_depth=15, n_estimators=200 ...............\n",
      "[CV]  learning_rate=1.0, max_depth=15, n_estimators=200, score=0.697, total=   0.5s\n",
      "[CV] learning_rate=1.0, max_depth=15, n_estimators=200 ...............\n",
      "[CV]  learning_rate=1.0, max_depth=15, n_estimators=200, score=0.684, total=   0.5s\n",
      "[CV] learning_rate=1.0, max_depth=15, n_estimators=200 ...............\n",
      "[CV]  learning_rate=1.0, max_depth=15, n_estimators=200, score=0.718, total=   0.5s\n",
      "[CV] learning_rate=1.0, max_depth=15, n_estimators=300 ...............\n",
      "[CV]  learning_rate=1.0, max_depth=15, n_estimators=300, score=0.702, total=   0.6s\n",
      "[CV] learning_rate=1.0, max_depth=15, n_estimators=300 ...............\n",
      "[CV]  learning_rate=1.0, max_depth=15, n_estimators=300, score=0.648, total=   0.6s\n",
      "[CV] learning_rate=1.0, max_depth=15, n_estimators=300 ...............\n",
      "[CV]  learning_rate=1.0, max_depth=15, n_estimators=300, score=0.695, total=   0.6s\n",
      "[CV] learning_rate=1.0, max_depth=15, n_estimators=300 ...............\n",
      "[CV]  learning_rate=1.0, max_depth=15, n_estimators=300, score=0.682, total=   0.6s\n",
      "[CV] learning_rate=1.0, max_depth=15, n_estimators=300 ...............\n",
      "[CV]  learning_rate=1.0, max_depth=15, n_estimators=300, score=0.722, total=   0.6s\n",
      "[CV] learning_rate=1.0, max_depth=15, n_estimators=400 ...............\n",
      "[CV]  learning_rate=1.0, max_depth=15, n_estimators=400, score=0.703, total=   0.7s\n",
      "[CV] learning_rate=1.0, max_depth=15, n_estimators=400 ...............\n",
      "[CV]  learning_rate=1.0, max_depth=15, n_estimators=400, score=0.648, total=   0.7s\n",
      "[CV] learning_rate=1.0, max_depth=15, n_estimators=400 ...............\n",
      "[CV]  learning_rate=1.0, max_depth=15, n_estimators=400, score=0.687, total=   0.7s\n",
      "[CV] learning_rate=1.0, max_depth=15, n_estimators=400 ...............\n",
      "[CV]  learning_rate=1.0, max_depth=15, n_estimators=400, score=0.680, total=   0.7s\n",
      "[CV] learning_rate=1.0, max_depth=15, n_estimators=400 ...............\n",
      "[CV]  learning_rate=1.0, max_depth=15, n_estimators=400, score=0.722, total=   0.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 180 out of 180 | elapsed:  3.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                     colsample_bylevel=None,\n",
       "                                     colsample_bynode=None,\n",
       "                                     colsample_bytree=None, gamma=None,\n",
       "                                     gpu_id=None, importance_type='gain',\n",
       "                                     interaction_constraints=None,\n",
       "                                     learning_rate=None, max_delta_step=None,\n",
       "                                     max_depth=None, min_child_weight=None,\n",
       "                                     missing=nan, monotone_constraints=None,\n",
       "                                     n_estimators=100, n_jobs=None,\n",
       "                                     num_parallel_tree=None, random_state=None,\n",
       "                                     reg_alpha=None, reg_lambda=None,\n",
       "                                     scale_pos_weight=None, subsample=None,\n",
       "                                     tree_method=None, validate_parameters=None,\n",
       "                                     verbosity=None),\n",
       "             param_grid={'learning_rate': [0.01, 0.1, 1.0],\n",
       "                         'max_depth': range(5, 20, 5),\n",
       "                         'n_estimators': range(100, 500, 100)},\n",
       "             verbose=3)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##parameters for the Grid-Search\n",
    "param_grid = {'n_estimators': range(100, 500, 100),\n",
    "              'learning_rate': [0.01, 0.1, 1.0],\n",
    "              'max_depth': range(5, 20, 5)\n",
    "              }\n",
    "\n",
    "## Cross validation is taking place with Grid-Search\n",
    "## 5-fold Cross validation has been done\n",
    "grid = GridSearchCV(XGBClassifier(), param_grid, refit = True, verbose = 3)\n",
    "grid.fit(data,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.1, 'max_depth': 15, 'n_estimators': 400}\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.1, max_delta_step=0, max_depth=15,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=400, n_jobs=0, num_parallel_tree=1, random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n"
     ]
    }
   ],
   "source": [
    "# print best parameter after tuning \n",
    "print(grid.best_params_) \n",
    "  \n",
    "# print how our model looks after hyper-parameter tuning \n",
    "print(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Test files DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('final_amino_acid_result_valid.csv', skiprows= lambda x: True if x%2 else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab = test_df.ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.index = test_df.ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.drop('ID',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1071, 20)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = clf.predict(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1071,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lab = pd.DataFrame([i for i in range(5000,5000+1072)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = pd.DataFrame(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = predict.replace(0,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       5000\n",
       "1       5001\n",
       "2       5002\n",
       "3       5003\n",
       "4       5004\n",
       "5       5005\n",
       "6       5006\n",
       "7       5007\n",
       "8       5008\n",
       "9       5009\n",
       "10      5010\n",
       "11      5011\n",
       "12      5012\n",
       "13      5013\n",
       "14      5014\n",
       "15      5015\n",
       "16      5016\n",
       "17      5017\n",
       "18      5018\n",
       "19      5019\n",
       "20      5020\n",
       "21      5021\n",
       "22      5022\n",
       "23      5023\n",
       "24      5024\n",
       "25      5025\n",
       "26      5026\n",
       "27      5028\n",
       "28      5029\n",
       "29      5030\n",
       "        ... \n",
       "1041    6070\n",
       "1042    6071\n",
       "1043    6072\n",
       "1044    6073\n",
       "1045    6074\n",
       "1046    6075\n",
       "1047    6076\n",
       "1048    6077\n",
       "1049    6078\n",
       "1050    6079\n",
       "1051    6080\n",
       "1052    6081\n",
       "1053    6082\n",
       "1054    6083\n",
       "1055    6084\n",
       "1056    6085\n",
       "1057    6086\n",
       "1058    6087\n",
       "1059    6088\n",
       "1060    6089\n",
       "1061    6090\n",
       "1062    6091\n",
       "1063    6092\n",
       "1064    6093\n",
       "1065    6094\n",
       "1066    6095\n",
       "1067    6096\n",
       "1068    6097\n",
       "1069    6098\n",
       "1070    6099\n",
       "Name: ID, Length: 1071, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.concat([lab,predict],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5000</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5001</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5002</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5003</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5004</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5005</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5006</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5007</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5008</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5009</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5010</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5011</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5012</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5013</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5014</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5015</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5016</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5017</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5018</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5019</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5020</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5021</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>5022</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5023</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>5024</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5025</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>5026</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>5028</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5029</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>5030</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1041</th>\n",
       "      <td>6070</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1042</th>\n",
       "      <td>6071</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1043</th>\n",
       "      <td>6072</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1044</th>\n",
       "      <td>6073</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1045</th>\n",
       "      <td>6074</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1046</th>\n",
       "      <td>6075</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1047</th>\n",
       "      <td>6076</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048</th>\n",
       "      <td>6077</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1049</th>\n",
       "      <td>6078</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1050</th>\n",
       "      <td>6079</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051</th>\n",
       "      <td>6080</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1052</th>\n",
       "      <td>6081</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1053</th>\n",
       "      <td>6082</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1054</th>\n",
       "      <td>6083</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1055</th>\n",
       "      <td>6084</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1056</th>\n",
       "      <td>6085</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1057</th>\n",
       "      <td>6086</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1058</th>\n",
       "      <td>6087</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1059</th>\n",
       "      <td>6088</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1060</th>\n",
       "      <td>6089</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1061</th>\n",
       "      <td>6090</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1062</th>\n",
       "      <td>6091</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1063</th>\n",
       "      <td>6092</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1064</th>\n",
       "      <td>6093</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1065</th>\n",
       "      <td>6094</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1066</th>\n",
       "      <td>6095</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1067</th>\n",
       "      <td>6096</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1068</th>\n",
       "      <td>6097</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1069</th>\n",
       "      <td>6098</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1070</th>\n",
       "      <td>6099</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1071 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID  0\n",
       "0     5000 -1\n",
       "1     5001  1\n",
       "2     5002  1\n",
       "3     5003  1\n",
       "4     5004 -1\n",
       "5     5005 -1\n",
       "6     5006 -1\n",
       "7     5007  1\n",
       "8     5008 -1\n",
       "9     5009 -1\n",
       "10    5010  1\n",
       "11    5011 -1\n",
       "12    5012 -1\n",
       "13    5013 -1\n",
       "14    5014  1\n",
       "15    5015  1\n",
       "16    5016 -1\n",
       "17    5017 -1\n",
       "18    5018  1\n",
       "19    5019 -1\n",
       "20    5020 -1\n",
       "21    5021  1\n",
       "22    5022  1\n",
       "23    5023  1\n",
       "24    5024  1\n",
       "25    5025  1\n",
       "26    5026  1\n",
       "27    5028 -1\n",
       "28    5029 -1\n",
       "29    5030  1\n",
       "...    ... ..\n",
       "1041  6070  1\n",
       "1042  6071 -1\n",
       "1043  6072  1\n",
       "1044  6073 -1\n",
       "1045  6074  1\n",
       "1046  6075 -1\n",
       "1047  6076  1\n",
       "1048  6077 -1\n",
       "1049  6078  1\n",
       "1050  6079  1\n",
       "1051  6080 -1\n",
       "1052  6081  1\n",
       "1053  6082 -1\n",
       "1054  6083 -1\n",
       "1055  6084 -1\n",
       "1056  6085 -1\n",
       "1057  6086 -1\n",
       "1058  6087  1\n",
       "1059  6088 -1\n",
       "1060  6089  1\n",
       "1061  6090 -1\n",
       "1062  6091 -1\n",
       "1063  6092  1\n",
       "1064  6093 -1\n",
       "1065  6094 -1\n",
       "1066  6095 -1\n",
       "1067  6096 -1\n",
       "1068  6097 -1\n",
       "1069  6098  1\n",
       "1070  6099  1\n",
       "\n",
       "[1071 rows x 2 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#snapshot of the output file \n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final output file in csv form\n",
    "output.to_csv('output7.csv', header=['ID','Lable'], index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
